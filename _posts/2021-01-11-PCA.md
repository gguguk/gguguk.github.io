---
title: PCA(주성분 분석)
author: Gukwon Koo
categories: [Math, Linear algebra]
tags: [pca]
pin: false
math: true
comments: true
date: 2021-01-11 11:28:00 +0900
---

![](https://i.imgur.com/Uv2dlsH.gif)_원래 데이터들을 어느 벡터에 사영시켜야 데이터의 구조를 최대한 보존할 수 있을까?_

<br>

### 3줄 요약

---

- PCA는 데이터의 **기존 구조를 최대한 유지**하고자 하는 목적을 가진 **변수 추출 또는 차원 축소** 기법임
- 데이터의 기존 구조를 최대한 유지 하기 위해서는 **사영(projection) 하였을 때 분산이 최대가 되는 방향**의 벡터를 찾아야 함
- 데이터의 **공분산 행렬의 고유벡터**가 분산이 최대가 되는 방향의 벡터임


---

<br>

### 본 글을 이해하기 위해 필요한 개념

---

- 내적
- 사영(projection)
- 대칭 행렬
- 공분산 행렬(covariance matrix)
- 고유값과 고유벡터
- 벡터 미분
- 라그랑주 승수법

---

<br>

앞선 글에서 [고유값과 고유벡터](https://gguguk.github.io/posts/eigenvalue_eigenvector/)에 대해 살펴 보았습니다.  본 글에서는 이를 활용한 대표적 변수 추출 또는 차원 축소 기법인 주성분 분석(PCA)에 관련된 내용을 정리하고자 합니다. 머신러닝을 공부해보신 분들은 한번쯤은 들어보셨을 텐데요. 매우 친숙한 알고리즘인데 반해 선형대수 기초 개념이 없으면 생각보다 이해하기 어려운 것도 사실입니다. 저와 함께 차근차근 알아가 보시죠. 본 글에서는 PCA의 개념을 살펴보고 PCA의 해를 찾는 방법, PCA의 가정과 활용 예시를 살펴보도록 하겠습니다.

<br>

본 글의 행렬, 벡터, 벡터의 원소의 표기는 다음을 따릅니다.

- 행렬: $$\boldsymbol{X}$$
- 벡터: $$\boldsymbol{x}$$
- 벡터의 원소: $$x_1, x_2, \cdots, x_i$$

<br>

# 1 &nbsp; PCA(주성분 분석)

---

## **개요**

PCA는 **주어진 데이터들의 구조를 최대한 보존할 수 있는 주성분(principal component)을 찾아주는 변수 추출(feature extraction) 기법**입니다. PCA는 비모수적 기법으로 분포적인 가정을 요구하지 않기 때문에 매우 다양한 분야에서 활용되고 있습니다. **주성분**과 **변수 추출**이라는 어려운 단어가 2개나 나왔네요. 먼저 이해하기 쉬운 것부터 살펴보겠습니다. 

### **변수 추출**

변수 추출이란 데이터가 가진 변수를 조합하여 새로운 변수를 만드는 기법을 말합니다. 예를들어 수학 점수와 영어 점수로 이루어진 데이터가 있다고 해봅사다. 수학 점수와 영어 점수를 잘 조작해서 새로운 변수를 만들 수도 있겠죠. 가장 간단하게는 $$ 0.5 \times $$수학점수 + $$0.5 \times$$영어 점수를 통해 새로운 변수를 만들 수도 있습니다. 이를 내적의 형태로 표현하면 다음과 같이 표현할 수 있습니다.

<br>

$$ \left[
    \begin{matrix} 
        \text{수학 점수} & \text{영어 점수}
		\end{matrix} 
\right]
\\
\left[
    \begin{matrix} 
        0.5 \\ \\
        0.5
		\end{matrix} 
\right]
= 0.5 \times \text{수학 점수} + 0.5 \times \text{영어 점수} $$ 

<br>

그렇다면 **어떤 벡터**에 데이터를 내적해야 기존 변수들을 잘 종합할 수 있을지에 대한 의문이 생깁니다. PCA는 이에 대한 답을 제공합니다. 그러나 막연하게 생각하면 어려우니까 **데이터의 구조 혹은 분포**를 활용해보자는 아이디어에서 출발하는 것이죠. 정리하자면 PCA는 어떻게 하면 데이터를 **잘 종합하는 변수를 만들 수 있을까**에 대한 솔루션인 셈입니다.

<br>

>  한편 PCA를 차원축소 기법으로도 많이 이야기 하는데, 이는 PCA를 수행하여 찾아낸 모든 고유벡터를 활용하지 않고 일부의 고유벡터만 활용한다면 데이터의 차원을 축소 할 수 있기 때문입니다. 고유벡터와 관련된 개념은 뒤에서 더 자세히 설명하겠습니다.

<br>

### **주성분(principal component)**

다음으로 주성분입니다. **주성분**이란 **데이터를 사영[^3] 시켰때 사영된 데이터들의 분산이 최대화 되는 벡터**를 말합니다. 글만 보면 이해하기 어렵습니다. 아래 두 이미지를 보시죠. 두 이미지는 파란색 점으로 표현된 데이터들을 검은색 실선으로 표현된 두개의 벡터에 각각 사영시켰습니다. 사영된 데이터들을 빨간색 점으로 나타냈는데요. 사영된 데이터들의 **분산이 최대**가 되는 (데이터가 더 넓게 퍼져 있는) **벡터**는 무엇인가요?

<br>

![](/assets/img/post_img/pca_variance.png){:width="650"}

<br>

왼쪽보다는 오른쪽 벡터가 빨간색 점들이 더 넓게 분포 되어 있음을 알 수 있습니다. 다시 말해 왼쪽 이미지는 분산이 낮고 오른쪽 이미지는 분산이 높다는 것이죠. 이렇게 기하학적으로 보았을 때 어느 쪽이 더 원래의 데이터의 형태를 잘 유지하고 있나요? 물론 두 이미지 모두 기존 데이터가 가진 정보는 일부 상실되었습니다. 그럼에도 불구하고 최대한 기존 구조를 유지하며 정보가 압축된 이미지는 어느 것인가요? 바로 오른쪽 이미지이죠.

주성분은 데이터를 사영시켰을 때 기존 구조를 최대한 잘 유지할 수 있게 하는 벡터를 뜻합니다. 데이터의 기존 구조가 최대한 잘 유지 된다는 뜻은 바로 사영 후의 분산이 최대가 되는 것이구요. 그렇다면 결국 PCA라는 알고리즘이 하고 싶은 것은 **데이터를 사영 후 분산이 최대가 되도록 하는 벡터를 찾는 일로 귀결**됨을 알 수 있습니다. 또한 예시는 2차원을 보여드렸지만 3차원 또는 그 이상에서도 같은 논리로 분산이 최대가 되는 벡터를 찾아 볼 수 있습니다.

<br>

# 2 &nbsp; PCA의 가정 및 특성



## **PCA의 가정**

PCA는 아래의 3가지 가정에 기반하고 있습니다. 그러나 이러한 가정이 항상 들어 맞지는 않겠죠. 데이터의 형태에 따라 PCA는 **실패(fail)**하기도 합니다([Shlens, J., 2014](https://arxiv.org/pdf/1404.1100&sa=U&ved=2ahUKEwi57Mfr0ZDpAhWtF6YKHfSxAck4ChAWMAZ6BAgEEAE&usg=AOvVaw2ccduDFnmcXvF-iGE-VXIM)). 

### Linearity

사영된 데이터들은 직선 위에 존재한다고 가정합니다. 그러나 아래의 이미지처럼 데이터들이 **곡선**을 따라 사영되었을 때 더욱 데이터의 구조를 잘 유지할 수 있는 경우도 있습니다. 기본적인 PCA는 이러한 비선형성을 잡아내지 못합니다. 따라서 비선형성을 반영할 수 있는 [non-linear PCA 기법](http://www.nlpca.org/)도 존재한다고 합니다.

![](https://www.researchgate.net/profile/Alberto_Bernacchia/publication/29619601/figure/fig3/AS:668906024865792@1536491032022/Two-Nonlinear-PCA-fits-Data-points-are-the-same-data-of-Fig-2-projected-along-the.png){:width="600"}_non-linear PCA의 예시 (출처: http://www.nlpca.org/)_

<br>

### Large variances have important structure

분산이 큰 방향이 데이터 구조의 중요한 정보를 가지고 있다고 가정합니다. 그러나 이러한 가정 역시 데이터의 구조에 따라 위배가 되는 경우가 있습니다. 아래의 그림은 놀이동산 관람차를 탄 사람들의 좌표값을 나타낸 그림입니다. 빨간색 축은 데이터의 분산이 가장 큰 방향의 축입니다. 그러나 기존 데이터를 종합한다는 의미로 볼때, 해당 축에 사영하는 것이 가장 좋은 선택일까요? 아래 그림에서는 오히려 관람차의 회전 각도 $$\theta$$라는 하나의 변수만 있으면 3차원 데이터를 1차원 스칼라 값으로 압축할 수 있습니다. 다시 말해 데이터를 압축하여 표현할 때 분산이 최대인 축을 선택하는 것이 능사가 아니라는 것입니다.

![](/assets/img/post_img/pca_fail_large_variance.png){:width="250"}_"A Tutorial on Principal Component Analysis", Jonathon Shlens_

<br>

### The principal components are orthogonal

주성분끼리는 항상 직교(orthogonal) 한다고 가정합니다. 이 가정 역시 모든 데이터에 항상 맞는 가정은 아닌데요. 아래의 그림을 보시면 빨간색 축이 데이터의 분산이 가장 큰 방향의 벡터입니다. 가로로 긴 첫번째 축은 데이터의 구조를 표현하기에 적절한 선택인 것 같습니다. 그러나 이 축과 직교하는 두번째 축은 non-orthogonal한 데이터의 특징을 잘 잡아내지 못합니다.

![](/assets/img/post_img/pca_fail_non_orthogonal_axes.png){:width="250"}_"A Tutorial on Principal Component Analysis", Jonathon Shlens_

<br>

# 3 &nbsp; PCA 해 찾기

---

이제 직접 사영시켰을 때 분산이 최대가 되는 벡터 $$\boldsymbol{v}$$를 찾아보죠 🙌 그전에 몇 가지 가정할 사항이 있습니다.

- 우리가 찾을 벡터 $$\boldsymbol{v}$$는 **단위 벡터**[^1]입니다
- 데이터 행렬 $$\boldsymbol{X}$$는 $$n$$개의 데이터, $$d$$개의 변수(feature)를 가진 $$n \times d$$ 행렬입니다.
- 데이터 행렬  $$\boldsymbol{X}$$의 각 변수들은 평균이 $$0$$으로 centering 되어있습니다. 다시 말해 데이터 행렬  $$\boldsymbol{X}$$의 $$k$$번째 열벡터 $$\boldsymbol{x}_k$$는 $$\boldsymbol{x}_k - E(\boldsymbol{x}_k)$$가 되어 있는 상태입니다.
- 앞으로 등장할 설명과 수식은 $$d$$차원의 데이터 $$\boldsymbol{X}$$를 1차원으로 차원 감소하는 상황을 가정한 예시들입니다.

<br>

$$n \times d$$ 데이터 행렬은 다음과 같이 표기합니다.

<br>

$$ \boldsymbol{X}
=\left[
    \begin{matrix} 
       x_{11} & \cdots & x_{1d} \\
        \vdots & \ddots & \vdots \\
        x_{n1} & \cdots & x_{nd}
    \end{matrix} 
\right] 
= 
\left[
    \begin{matrix} 
       | & \cdots & | \\
        \boldsymbol{x}_1 & \ddots & \boldsymbol{x}_d \\
        | & \cdots & |
    \end{matrix} 
\right] $$

<br>

자, 이제 우리는 **데이터 행렬 $$X$$를 어떤 벡터 $$\boldsymbol{v}$$에 사영해야 사영된 데이터들의 분산이 최대**가 될지를 찾아야 합니다. 이는 **데이터 행렬 $$\boldsymbol{X}$$의 행벡터들과 열벡터 $$\boldsymbol{v}$$를 내적**하는 것으로 구할 수 있습니다. 사영과 내적이 어떤 관련이 있는걸까요? 우리는 앞서 벡터 $$\boldsymbol{v}$$를 단위 벡터라고 가정했습니다. 따라서 어떤 벡터의 단위 벡터에 대한 정사영 벡터는 두 벡터를 내적하는 것만으로도 도출할 수 있습니다[^5] 사영과 관련된 내용은 [이 글](https://ratsgo.github.io/linear%20algebra/2017/10/20/projection/)을 추천드립니다. 행벡터와 열벡터 $$\boldsymbol{v}$$의 내적을 일일이 진행할 수 없으니 matrix notation을 통해 표현해 보죠. 참고로 $$\boldsymbol{X}\boldsymbol{v}$$ 는 $$n \times 1$$ 크기를 가지는 열벡터입니다.

<br>

$$ \boldsymbol{X}\boldsymbol{v}
=\left[
    \begin{matrix} 
       x_{11} & \cdots & x_{1d} \\
        \vdots & \ddots & \vdots \\
        x_{n1} & \cdots & x_{nd}
    \end{matrix} 
\right]
\left[
	\begin{matrix}
		v_1 \\
		\vdots \\
		v_d
	\end{matrix}
\right] $$

<br>

데이터 $$\boldsymbol{X}$$를 어떤 단위 벡터 $$\boldsymbol{v}$$에 사영하는 것은 $$\boldsymbol{Xv}$$ 를 계산하는 것과 같다는 것을 알게되었습니다. 그럼 사영된 데이터의 분산이 최대가 되는 벡터 $$\boldsymbol{v}$$를 찾는 것을 수식으로 표현해볼까요?

<br>

$$ \DeclareMathOperator*{\argmax}{argmax}\argmax_{\boldsymbol{v}}\Big[Var(\boldsymbol{X}\boldsymbol{v})\Big] $$

<br>

이제 $$Var(\boldsymbol{X}\boldsymbol{v})$$를 전개해 나가 보겠습니다.

<br>

$$ \begin{align}
	Var(\boldsymbol{X}\boldsymbol{v}) \tag{1}
	&= E[(\boldsymbol{X}\boldsymbol{v})^2] - E[(\boldsymbol{X}\boldsymbol{v})]^2 \\[1em] \tag{2}
	&= \cfrac{1}{n} \sum_{i=1}^{n}x_i^2 - \bigg(\cfrac{1}{n}\sum_{i=1}^{n}x_i \bigg)^2 \\[1em] \tag{3}
	&= \cfrac{1}{n} \sum_{i=1}^{n}x_i^2 \\[1em] \tag{4}
	&= \cfrac{1}{n} (\boldsymbol{X}\boldsymbol{v})^{T} (\boldsymbol{X}\boldsymbol{v}) \\[1em] \tag{5}
	&= \cfrac{1}{n} \boldsymbol{v}^{T}\boldsymbol{X}^{T}\boldsymbol{X}\boldsymbol{v} \\[1em] \tag{6}	&= \boldsymbol{v}^{T} \cfrac{\boldsymbol{X}^{T}\boldsymbol{X}}{n} \boldsymbol{v} \\[1em] \tag{7}
	&= \boldsymbol{v}^{T} \Sigma \boldsymbol{v}
\end{align} $$

<br>

$$(2)$$의 $$\bigg(\cfrac{1}{n}\sum_{i=1}^{n}x_i \bigg)^2$$은 $$0$$이 되어 사라지는 term입니다. 그 이유는 우리가 데이터 행렬 $$\boldsymbol{X}$$의 열벡터들이 모두 평균 0으로 centering 되어 있다고 가정했기 때문입니다. 직접 숫자를 대입해 계산 해보시면 쉽게 이해가 가실겁니다. 또한 $$(6)$$의 $$\cfrac{\boldsymbol{X}^{T}\boldsymbol{X}}{n}$$는 [공분산 행렬](https://angeloyeo.github.io/2019/07/27/PCA.html#fnref:6)을 나타내는데요. 이 역시도 데이터 행렬 $$\boldsymbol{X}$$의 열벡터들이 모두 평균 0으로 centering 되어 있다고 가정했기 때문입니다.

<br>

목적식을 $$\boldsymbol{v}^{T} \Sigma \boldsymbol{v}$$, 제약식을 $$ \|\boldsymbol{v}\|= \boldsymbol{v}^T\boldsymbol{v} = 1 $$으로 두고 라그랑주 승수법[^4]을 이용하여 다음의 보조 방정식 $$L$$을 만들 수 있습니다. 제약식은 $$\boldsymbol{v}$$를 단위벡터로 가정한 결과로 도출되었습니다.

<br>

$$ L = \boldsymbol{v}^{T} \Sigma \boldsymbol{v} - \lambda(\|\boldsymbol{v}\|-1) $$

<br>

최대값을 구하기 위해 $$L$$을 $$\boldsymbol{v}$$에 대해 편미분한 식을 $$0$$으로 두고 전개하겠습니다. 벡터의 미분 공식은 [이 글](https://darkpgmr.tistory.com/141)을 참고해주세요.

<br>

$$ \cfrac{\partial L}{\partial \boldsymbol{v}} = 2\Sigma\boldsymbol{v} - 2\lambda\boldsymbol{v} = 0 $$

<br>

최종적으로 아래의 식을 얻을 수 있습니다.

<br>

$$\Sigma\boldsymbol{v} = \lambda\boldsymbol{v}$$

<br>

어딘가 익숙한 수식 아닌가요? 어떤 벡터에 선형변환 행렬을 곱했더니 크기는 변하지만 방향이 변하지 않는 벡터가 있다는 뜻이네요. 바로 [고유값과 고유벡터](https://gguguk.github.io/posts/eigenvalue_eigenvector/)에 관한 수식임을 알 수 있습니다. 

따라서 PCA가 하는 일은 **데이터 행렬 $$\boldsymbol{X}$$의 공분산 행렬 $$\Sigma$$의 고유 벡터를 찾는 것**입니다. 또한 우리가 수식을 전개해와서 알 수 있듯이 **그 고유 벡터가 데이터들의 사영 후 분산을 최대화 시키는 벡터**입니다. 또한 우리는 고유벡터끼리의 직교성을 가정하였는데요. [공분산 행렬의 고유벡터는 서로 직교하는 성질](https://ratsgo.github.io/machine learning/2017/04/24/PCA)이 (정확하게는 대칭행렬[^6]의 특성) 있기 때문에 가정에 위배 되지 않습니다.

고유벡터는 최대 데이터 행렬 $$\boldsymbol{X}$$의 차원 수인 $$d$$개 만큼 존재할 수 있습니다. 따라서 $$d$$개의 고유벡터를 활용한다면 데이터의 차원은 $$d$$차원으로 유지할 수 있으면서도 각 열벡터들의 상관 관계를 제거할 수 있는 결과를 가져다 줍니다 (공분산 행렬의 고유벡터들은 서로 orthogonal 하기 때문입니다 ). 또한 고유벡터를 $$k(\lt d)$$개 만큼만 사용한다면 $$d$$차원을 $$k$$차원으로 차원축소 할 수 있게 됩니다.

<br>

# 4 &nbsp; PCA 활용 방법

## 차원 축소를 통한 시각화

아래의 이미지처럼 3차원 공간에 존재하는 데이터들을 2개의 고유벡터에 사영시키면 2차원의 데이터로 차원을 축소할 수 있습니다. 따라서 고차원의 데이터를 우리가 쉽게 상상할 수 있는 2차원이나 3차원 데이터로 표현하므로써 데이터에 대한 기하학적 해석을 용이하게 할 수 있습니다.

![](/assets/img/post_img/pca_3d_to_2d.jpg){:width="650"}_3차원 데이터를 2차원으로 축소한 예시_



## 선형 회귀의 데이터로 활용

공분산 행렬의 고유 벡터들을 서로 직교(orthogonal) 하는 특성이 있습니다 (정확하게는 대칭행렬[^6]의 특성입니다). [직교하는 벡터는 상관관계가 0](https://angeloyeo.github.io/2019/08/20/correlation_and_inner_product.html)이기 때문에 서로 다른 고유 벡터에 사영하여 얻어진 벡터들은 상관관계가 없어집니다. 다시 말해 correlation이 있는 변수들을 decorrelation 하게 변환시켜줄 수 있다는 의미이죠. 따라서 PCA를 통한 변수 추출을 통해 다중공선성 문제에서 벗어날 수 있습니다.

<br>

# 참고자료

---

- [주성분 분석(PCA), 공돌이의 수학정리노트 ](https://angeloyeo.github.io/2019/07/27/PCA.html#fnref:6)
- [주성분분석(Principal Component Analysis), ratsgo's blog](https://ratsgo.github.io/machine learning/2017/04/24/PCA/)
- [주성분분석(PCA)의 이해와 활용, 다크 프로그래머](https://darkpgmr.tistory.com/110)
- [PCA (Principal Component Analysis): 주성분분석에 대한 모든 것! , 통계학에 관한 모든것](http://blog.naver.com/PostView.nhn?blogId=sw4r&logNo=221031465518&parentCategoryNo=&categoryNo=43&viewDate=&isShowPopularPosts=true&from=search)
- [Principal Components: Mathematics, Example, Interpretation](http://www.stat.cmu.edu/~cshalizi/350/lectures/10/lecture-10.pdf)
- [An Introduction to Principal Component Analysis (PCA) with 2018 World Soccer Players Data, Kan Nishida](https://blog.exploratory.io/an-introduction-to-principal-component-analysis-pca-with-2018-world-soccer-players-data-810d84a14eab)
- [Shlens, J. (2014). A tutorial on principal component analysis. *arXiv preprint arXiv:1404.1100*.](https://arxiv.org/pdf/1404.1100&sa=U&ved=2ahUKEwi57Mfr0ZDpAhWtF6YKHfSxAck4ChAWMAZ6BAgEEAE&usg=AOvVaw2ccduDFnmcXvF-iGE-VXIM)
- [고유값 분해와 뗄래야 뗄 수 없는 주성분분석, bskyvision](https://bskyvision.com/347)
- [Matrix calculus, wikipedia](https://en.wikipedia.org/wiki/Matrix_calculus)
- [벡터 미분과 행렬 미분, 다크 프로그래머](https://darkpgmr.tistory.com/141)
- [사영(projection), ratsgo's blog](https://ratsgo.github.io/linear%20algebra/2017/10/20/projection/)
- [대각화와 고유값 분해](http://elearning.kocw.net/contents4/document/lec/2013/Chungbuk/LeeGeonmyeong1/12.pdf)
- [라그랑주 승수법, 위키피디아](https://ko.wikipedia.org/wiki/%EB%9D%BC%EA%B7%B8%EB%9E%91%EC%A3%BC_%EC%8A%B9%EC%88%98%EB%B2%95)

<br>

# footnote

[^1]: 길이가 1인 벡터를 뜻합니다. $$ \|v\|=1 $$
[^2]: http://www.nlpca.org/
[^3]: 도형의 각 점에서 한 평면에 내린 수선의 발을 의미합니다.
[^4]: 주어진 제약 조건 하에서 최댓값이나 최솟값을 찾는 최적화 문제를 푸는 방법입니다.
[^5]: 벡터 $$\boldsymbol{k}$$를 벡터 $$\boldsymbol{v}$$에 사영하여 얻을 수 있는 벡터는 $$c\boldsymbol{v}$$로 나타낼 수 있습니다 ($$c$$는 스칼라). 이때 $$c=\cfrac{\boldsymbol{k}^T \boldsymbol{v}}{\boldsymbol{v}^T \boldsymbol{v}}$$입니다. 만일 $$\boldsymbol{v}$$가 단위벡터라면 $$\boldsymbol{v}^T\boldsymbol{v}=1$$ 이므로 $$c=\boldsymbol{k}^T\boldsymbol{v}$$, 즉 두 벡터의 내적을 통해서 구할 수 있습니다.
[^6]: 정방행렬(square matrix)이면서 $$\boldsymbol{A} = \boldsymbol{A}^T$$를 만족하는 행렬을 뜻합니다.


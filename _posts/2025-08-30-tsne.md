---
title: T-SNE 이해하기
author: Gukwon Koo
categories: [ML, Basic]
tags: [t-sne, kl-divergence, gaussian-distribution, t-distribution]
pin: false
math: true
comments: false
date: 2025-08-30 14:52:00 +0900
---



T-SNE는 고차원의 임베딩을 저차원으로 변환하고 시각화화여 임베딩을 품질들 대략적으로 파악하는 것을 돕는 대표적인 차원 축소 알고리즘입니다. 실제로 아직도 현업에서도 많이 사용되는 알고리즘이고, 여러 논문에서도 임베딩의 질을 시각적으로 보여주기 위한 방법으로 많이 활용되고 있습니다.

그러나 어느 순간 T-SNE의 원리를 완벽하게 이해하지 못하고 사용하고 있다는 생각이 들었습니다. 🫠 앞으로 현업에서 임베딩을 다룰 일이 많아질 것 같은 느낌이 들어서 이번 기회에 정확한 동작 원리를 공부해보았습니다. 공부해보니 ML 기초 이론을 복습하기에도 좋은 알고리즘이라는 생각이 들더라구요. 앞으로 설명 드릴 내용은 다음의 배경 지식을 요구합니다.

- Gaussian distribution
- T distribution
- KL-Divergence
- Gradient decent
- (옵션) PCA

<br>

# T-SNE = Gaussian + T + KL-Divergence + Gradient Decent

T-SNE의 핵심 작동 원리는 4개의 키워드로 표현할 수 있습니다.

- **Gaussian distribution**: 고차원 공간(원래 벡터들이 존재하는 공간)을 가우시안 분포로 표현합니다. 이때 원래 가까운 벡터들은 높은 확률값을 갖도록, 그 반대는 반대가 되도록 합니다.
- **T distribution**: 저차원 공간(시각화를 위한 공간, 일반적을 2~3차원)을 T 분포로 표현합니다. 이때 고차원에서 원래 가까운 벡터들은 높은 확률을 가지도록, 그 반대는 반대가 되도록 표현하는 T 분포를 찾는 것이 목표입니다.
- **KL-Divergence**: 가우시안 분포와 T 분포가 차이가 나는 정도를 계산합니다.
- **Gradient Decent**: KL-Divergence의 값에 대한 저차원 벡터의 gradient를 계산하고, KL-Divergence가 작아지는 방향으로 저차원 벡터를 점진적으로 이동시킵니다.

<br>

##  예시로 학습 방식 직관적으로 이해해 보기

T-SNE의 학습 방식을 예시를 들어봅시다. 직관적으로 이해하기 쉬울 거예요. T-SNE는 먼저 고차원에서 각 벡터(예: 학생 A, B)가 서로 **얼마나 가까운지를 계산**합니다.

- 가까우면: 이 둘은 서로 비슷하네
- 멀면: 이 둘은 서로 다르네



같은 말을 **확률**로 표현하면 이렇게도 표현할 수 있습니다.

- 가까우면: 이 둘이 서로 친구일 확률은 0.8
- 멀면: 이 둘이 서로 친구일 확률은 0.1



이제 벡터를 저차원 공간(예: 2차원)에 배치하는데, 이 때 **위에서 계산한 친구일 확률을 최대한 유지하는 것**이 중요합니다.

- 고차원에서 **가까웠다면**: 저차원 공간에서도 서로 친구일 확률을 **크게**
- 고차원에서 **멀었다면**: 저차원 공간에서도 서로 친구일 확률을 **작게**



고차원 공간의 확률과 저차원 공간의 확률 구조를 비슷하게 유지하기 위해서 **벌점을 부과**합니다.

- 고차원에서 가까웠지만(확률 큼) 저차원에서 멀면(확률 작음): 벌점이 커짐
- 고차원에서 가까웠고(확률 큼) 저차원에서도 가까우면(확률 큼): 벌점이 작아짐



마지막으로 이 **벌점을 줄이기 위한 방향**으로 **저차원 공간의 벡터들을 점진적으로 이동**시킵니다.

- 벌점이 큰 벡터: 벌점이 작아지는 방향으로 크게 이동 시킴
- 벌점이 작은 벡터: 벌점이 작아지는 방향으로 작게 이동 시킴

<br>

## 수식과 함께 더 자세히 이해해 보기

T-SNE의 학습 방식을 예시를 통해 직관적으로 이해해 보았습니다. 이제 수식을 곁들여서 좀 더 자세히 이해해 보죠!



### 고차원 확률 분포 정의 - 가우시안 분포

T-SNE는 벡터 $$i$$와 $$j$$가 있을 때($$i \neq j$$) $$i$$와 $$j$$의 유사도(거리)에 비례하는 확률 분포 $$p_{ij}$$를 계산하는 것이 첫번째 목표입니다. 이를 위해 $$i$$가 $$j$$를 이웃으로 선택할 확률 $$p_{j|i}$$를 조건부 확률로 나타냅니다. 이 때, $$p_{i|i} = 0$$이고 $$\sum_{j}P_{j|i}=1, \forall i$$입니다.


$$
p_{j|i} = \frac{\exp\!\left(-\| \mathbf{x}_i - \mathbf{x}_j \|^2 / 2\sigma_i^2 \right)}
{\sum_{k \neq i} \exp\!\left(-\| \mathbf{x}_i - \mathbf{x}_k \|^2 / 2\sigma_i^2 \right)}
$$


이 수식을 가만히 살펴보면, 다변량 확률 분포인지 아닌지에 의해서 수식의 형태만 살짝 달라졌을 뿐이지 $$\mathbf{x}_i$$를 **평균**으로 하는 **가우시안 분포**의 수식과 사실상 같다는 것을 알 수 있습니다.


$$
N(x; \mu, \sigma^2) = \cfrac{1}{\sqrt{2\pi\sigma^2}} \exp(- (x-\mu)^2/2\sigma^2)
$$


즉, 고차원 공간에서의 벡터의 거리를 확률로 변환하는데 이때 가우시안 분포를 활용한다는 것을 알 수 있습니다. 또한 $$p_{j|i}$$의 분모의 수식에 의해 식이 노말라이즈 되면서 $$\sum{j}P_{j|i}=1 \forall i$$가 만족될 수 있음을 알 수 있습니다.



마지막으로 조건부 확률을 대칭 확률로 변환합니다. 이는 $$i$$와 $$j$$가 서로 이웃일 확률 정도로 해석하면 됩니다.


$$
p_{ij} = \cfrac{p_{j|i} + p_{i|j}}{2N}
$$






## 시각적으로 이해해 보기



## FAQ

- 디멘젼이 다른 분포들 간에 KL-Divergence를 계산할 수 있나?



# 하이퍼파라미터

- perplexity





# PCA랑 뭐가 다를까?

- PCA: 선형 관계만 표현 가능. 분산이 가장 크게 되는 고유벡터에 사상
- T-SNE: 비선형 관계도 표현 가능



# 참고자료

- [Normal distribution](https://en.wikipedia.org/wiki/Normal_distribution)
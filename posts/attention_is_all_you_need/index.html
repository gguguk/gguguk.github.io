<!DOCTYPE html><html lang="en" mode="light" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta http-equiv="Cache-Control" content="no-cache, no-store, must-revalidate"><meta http-equiv="Expires" content="0"><meta http-equiv="Pragma" content="no-cache"><meta name="day-prompt" content="days ago"><meta name="hour-prompt" content="hours ago"><meta name="minute-prompt" content="minutes ago"><meta name="justnow-prompt" content="just now"><meta name="generator" content="Jekyll v4.4.1" /><meta property="og:title" content="Attention Is All You Need" /><meta name="author" content="Gukwon Koo" /><meta property="og:locale" content="en" /><meta name="description" content="“Attention Is All You Need” 논문을 읽은 후 관련 자료를 정리한 내용을 바탕으로 논문 리뷰를 진행 해보겠습니다. 서론 및 문헌연구는 제외하겠습니다." /><meta property="og:description" content="“Attention Is All You Need” 논문을 읽은 후 관련 자료를 정리한 내용을 바탕으로 논문 리뷰를 진행 해보겠습니다. 서론 및 문헌연구는 제외하겠습니다." /><link rel="canonical" href="https://gguguk.github.io/posts/attention_is_all_you_need/" /><meta property="og:url" content="https://gguguk.github.io/posts/attention_is_all_you_need/" /><meta property="og:site_name" content="생각과 고민." /><meta property="og:type" content="article" /><meta property="article:published_time" content="2020-10-16T00:00:00+09:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Attention Is All You Need" /><meta name="twitter:site" content="@twitter_username" /><meta name="twitter:creator" content="@Gukwon Koo" /><meta name="google-site-verification" content="google_meta_tag_verification" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Gukwon Koo"},"dateModified":"2020-10-16T00:00:00+09:00","datePublished":"2020-10-16T00:00:00+09:00","description":"“Attention Is All You Need” 논문을 읽은 후 관련 자료를 정리한 내용을 바탕으로 논문 리뷰를 진행 해보겠습니다. 서론 및 문헌연구는 제외하겠습니다.","headline":"Attention Is All You Need","mainEntityOfPage":{"@type":"WebPage","@id":"https://gguguk.github.io/posts/attention_is_all_you_need/"},"url":"https://gguguk.github.io/posts/attention_is_all_you_need/"}</script><title>Attention Is All You Need | 생각과 고민.</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="생각과 고민."><meta name="application-name" content="생각과 고민."><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column align-items-end" lang="en"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="/assets/img/sample/bear.png" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">생각과 고민.</a></div><div class="site-subtitle font-italic">주니어 데이터 사이언티스트입니다.</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tags ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center"> <a href="https://github.com/gguguk" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github-alt"></i> </a> <a href="https://www.linkedin.com/in/%EA%B5%AD%EC%9B%90-%EA%B5%AC-32a9691a1/" aria-label="linkedin" target="_blank" rel="noopener"> <i class="fab fa-linkedin"></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>Attention Is All You Need</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>Attention Is All You Need</h1><div class="post-meta text-muted d-flex flex-column"><div> <span class="semi-bold"> Gukwon Koo </span> on <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="Fri, Oct 16, 2020, 12:00 AM +0900" >Oct 16, 2020<i class="unloaded">2020-10-16T00:00:00+09:00</i> </span></div><div> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="1372 words">7 min read</span></div></div><div class="post-content"><blockquote><p>“Attention Is All You Need” 논문을 읽은 후 관련 자료를 정리한 내용을 바탕으로 논문 리뷰를 진행 해보겠습니다. 서론 및 문헌연구는 제외하겠습니다.</p></blockquote><h2 id="1---model-architecture">1     Model Architecture</h2><p>transformer에서 가장 중요한 block은 <code class="language-plaintext highlighter-rouge">self-attention</code>과 <code class="language-plaintext highlighter-rouge">point-wise feed forward network</code> sub-layer입니다. 인코더와 디코더 셀 하나는 self-attention 및 point-wise feed forward network 2개로 구성되어 있습니다. 각 sub-layer는 residual connection와 layer batch normalization으로 연결되어 있습니다.</p><h3 id="11---encoder-and-decoder-stacks">1.1     Encoder and Decoder Stacks</h3><p>Transformer imitates the classical attention mechanism (known e.g. from Bahdanau et al., 2014 or Conv2S2) where in encoder-decoder attention layers queries are form previous decoder layer, and the (memory) keys and values are from output of the encoder. Therefore, each position in decoder can attend over all positions in the input sequence. Decoder acts similarly generating one word at a time in a left-to-right-pattern. It attends to previously generated words of decoder and final representation of encoder.</p><h3 id="12--positional-encoding">1.2   Positional Encoding</h3><p>transformer는 sequence token의 sequence 정보를 알 수 없으므로, 순서 정보를 주입(inject) 시켜주어야 합니다.</p><h3 id="13-scaled-dot-product-attention">1.3   Scaled Dot-Product Attention</h3>\[\begin{aligned} Attention(Q, K, V) = softmax\Bigg(\cfrac{QK^{T}}{\sqrt{d_{k}}}\Bigg)V \end{aligned}\]<p>왜 \(\sqrt{d_{k}}\)로 나누어 줄까요? \(d_{k}\)가 커짐에 따라 dot-product가 너무 커져버리는 효과를 상쇄하기 위함입니다. 예를 들어 아래의 두 벡터를 비교해봅시다.</p>\[\begin{aligned} \vec{a} &amp;=[2, 2], \space where \space d_{k}=2 \\ \vec{b} &amp;=[2, 2, 2, 2, 2], \space where \space d_{k}=5 \\ \end{aligned}\] \[\begin{aligned} \vec{a} \cdot \vec{a} &amp;= 8\\ \vec{b} \cdot \vec{b} &amp;= 20, \end{aligned}\]<p>\(d_k\), 즉 임베딩 차원이 길 수록, dot-product 값은 커지는 필연적인 결과가 나타납니다. dot-product 값이 커지면 어떤 문제가 생길까요? 결론적으로 말하면 특정 노드의 softmax값이 1에 수렴하게 되어 gradient가 매우 작아지므로 초기 학습시에 문제가 발생할 여지가 있습니다. 저자는 논문에 이렇게 표현하고 있습니다.</p><blockquote><p>We suspect that for large values of \(d_{k}\), the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients . To counteract this effect, we scale the dot products by \(\cfrac{1}{\sqrt{d_{k}}}\)</p></blockquote><p>간단하게 코드로 알아봅시다. \(0.3\)과 \(0.4\) 두 개의 원소를 가지고 있는 \(d_{k}=2\)인 \(\vec{v_{1}}\)를 만들고 softmax 함수의 값을 구해봅시다.</p><div class="language-python highlighter-rouge"><div class="code-header" text-data="python"><button data-original-title="Copied!"><i class="far fa-clone"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre><td class="rouge-code"><pre><span class="p">[</span><span class="n">In</span><span class="p">]:</span>
<span class="n">v1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">])</span>
<span class="nf">print</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">v1</span><span class="p">)</span><span class="o">/</span><span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">v1</span><span class="p">)))</span>

<span class="p">[</span><span class="n">Out</span><span class="p">]:</span> 
<span class="nf">array</span><span class="p">([</span><span class="mf">0.47502081</span> <span class="mf">0.52497919</span><span class="p">])</span>
</pre></table></code></div></div><p>이제 \(\vec{v_{1}}\)의 각 원소에 100을 곱한 \(\vec{v_{2}}\)에 역시 softmax 함수를 통과한 값을 구해봅시다. \(\vec{v_{1}}\)과 \(\vec{v_{2}}\)는 절대적 크기는 달라졌지만, <strong><em>상대적 크기는 달라지지 않았다는 점</em></strong>을 유념해 주시기 바랍니다.</p><div class="language-python highlighter-rouge"><div class="code-header" text-data="python"><button data-original-title="Copied!"><i class="far fa-clone"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre><td class="rouge-code"><pre><span class="p">[</span><span class="n">In</span><span class="p">]:</span>
<span class="n">v2</span> <span class="o">=</span> <span class="n">v1</span> <span class="o">*</span> <span class="mi">100</span>
<span class="nf">print</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">v2</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">v2</span><span class="p">)))</span>

<span class="p">[</span><span class="n">Out</span><span class="p">]:</span>
<span class="nf">array</span><span class="p">([</span><span class="mf">4.53978687e-05</span> <span class="mf">9.99954602e-01</span><span class="p">])</span>
</pre></table></code></div></div><p>결과를 보면, 원소의 magnitude가 큰 벡터에 softmax를 씌웠을 때는 1에 수렴하는 값이 발생함을 할 수 있습니다. 따라서 초기에 학습이 잘 이루어지기 위해서는 gentle한 softmax 값을 얻어낼 필요가 있다는 것을 알 수 있습니다. 큰 softmax 값에 대해서는 graident가 매우 작아 역전파시 전달되는 값이 거의 0에 가깝기 때문입니다.</p><p>왜 softmax 값이 크면 전달되는 gradient가 작아질까요?(<em>small gradient problem</em>) 먼저 notation을 정의합시다.</p>\[\begin{aligned} z_j &amp;= element_{j}\text{ of output vector}\\ y_i &amp;= \cfrac {e^{z_i}} {\sum_{k}e^{z_i}} \end{aligned}\]<p>softmax 함수에 대한 미분을 구하면 아래와 같습니다(자세한 계산 과정은 Appendix를 참조해 주세요).</p>\[\cfrac {\partial {y_{i}}} {\partial {z_{j}}} = \begin{cases} y_{i}(1-y_{i}) &amp;\text{if } i=j \\ -y_{i}y_{j} &amp;\text{if } i\not=j \end{cases}\]<p>만약 \(i=j\)이고, \(y_i\)의 값이 1에 가깝다면, gradient는 0에 가까운 값이 전달 됩니다. 한편 \(i \not = j\)이고 \(y_i\)의 값이 1에 가깝다면, \(y_j\)는 0에 가까운 값이므로 역시 gradient 값이 매우 작아질 것입니다. 이러한 현상이 학습 말미에 벌어진다면 학습이 잘 되었다는 증거로 생각할 수 있겠으나, 학습 초기부터 small gradient 문제가 발생한다면 학습이 정상적으로 이루어지지 않을 것이라는 것을 미루어 짐작할 수 있습니다. 따라서 학습 초기에는 gentle softmax를 만드는 것이 중요하므로 \(\sqrt d_k\)로 나누어 주는 것입니다.</p><h3 id="14---multi-head-attention">1.4     Multi-Head Attention</h3><p><img data-proofer-ignore data-src="https://images.velog.io/images/gwkoo/post/dc43f85c-0663-4f72-a532-0b4c3e62bafc/image.png" alt="" /></p><p>The encoder self-attention distribution for the word “it” from the 5th to the 6th layer of a Transformer trained on English to French translation (one of eight attention heads).</p><h3 id="15--position-wise-feed-forward-networks">1.5   Position-wise Feed-Forward Networks</h3><p>self-attention layer를 통과한 결과에 feed-forward network를 붙여주는 구간입니다. 이는 네트워크에 비선형성을 전달하기 위한 과정이라고 보시면되겠습니다. self-attention layer에는 비선형성을 만들만한 연산이 포함되어 있지 않습니다.</p><h3 id="16--encoder-decoder-attention">1.6   Encoder-Decoder Attention</h3><p>mask-attention layer가 필요한 부분입니다.</p><p><br /></p><h2 id="2--appendix">2   Appendix</h2><h3 id="21--derivative-of-softmax">2.1   Derivative of softmax</h3>\[\begin{aligned} \text{If } i=j, \\ \cfrac{\partial y_{i}}{\partial z_{i}} &amp;= \cfrac{\partial}{\partial z_{i}} \bigg(\cfrac{e^{z_{i}}}{\sum_{k}e^{z_{k}}}\bigg) \\ &amp;= \cfrac {e^{z_{i}}\sum_{k}e^{z_{k}}-e^{z_{i}}e^{z_{i}}}{(\sum_{k}e^{z_{k}})^{2}} \\ &amp;= \cfrac {e^{z_{i}}(\sum_{k}e^{z_{k}}-e^{z_{k}})} {(\sum_{k}e^{z_{k}})^{2}} \\ &amp;= \cfrac {e^{z_{i}}} {\sum_{k}e^{z_{k}}} * \cfrac {\sum_{k}e^{z_{k}}-e^{z_{i}}} {\sum_{k}e^{z_{k}}} \\ &amp;= \cfrac {e^{z_{k}}} {\sum_{k}e^{z_{k}}}(1-\cfrac {e^{z_{k}}} {\sum_{k}e^{z_{k}}}) \\ &amp;= y_{i}(1-y_{i}) \end{aligned}\] \[\begin{aligned} \text{If } i \not= j, \\ \cfrac{\partial y_{i}}{\partial z_{j}} &amp;= \cfrac{\partial}{\partial z_{i}} \bigg(\cfrac{e^{z_{j}}}{\sum_{k}e^{z_{k}}}\bigg) \\ &amp;= \cfrac {0*\sum_{k}e^{z_{k}}-e^{z_{i}}e^{z_{j}}}{(\sum_{k}e^{z_{k}})^{2}} \\ &amp;= - \cfrac {e^{z_i}}{\sum_k e^{z_{k}}} * \cfrac {e^{z_j}}{\sum_k e^{z_{k}}} \\ &amp;= -y_i y_j \end{aligned}\]<p><br /></p><h2 id="3--참고자료">3   참고자료</h2><ul><li><a href="https://ratsgo.github.io/deep%20learning/2017/10/02/softmax/">Softmax-with-Loss 계층</a><li><a href="https://glassboxmedicine.com/2019/08/15/the-transformer-attention-is-all-you-need/">The Transformer: Attention Is All You Need</a><li><a href="https://www.tensorflow.org/tutorials/text/transformer">Transformer model for language understanding-tensorflow</a><li><a href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html">Google AI Blog - Transformer: A Novel Neural Network Architecture for Language Understanding</a><li><a href="https://wikidocs.net/24996">딥러닝을 통한 자연어처리 입문: 시퀀스-투-시퀀스(Sequence-to-Sequence, seq2seq)</a><li><a href="https://github.com/jadore801120/attention-is-all-you-need-pytorch">A PyTorch implementation of the Transformer model in “Attention is All You Need”.</a><li><a href="http://docs.likejazz.com/bert/#masked-attention">BERT 톺아보기</a></ul></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/ml/'>ML</a>, <a href='/categories/paper-review/'>Paper Review</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/nlp/" class="post-tag no-text-decoration" >nlp</a> <a href="/tags/paper-review/" class="post-tag no-text-decoration" >paper review</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=Attention Is All You Need - 생각과 고민.&url=https://gguguk.github.io/posts/attention_is_all_you_need/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=Attention Is All You Need - 생각과 고민.&u=https://gguguk.github.io/posts/attention_is_all_you_need/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=Attention Is All You Need - 생각과 고민.&url=https://gguguk.github.io/posts/attention_is_all_you_need/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" title-succeed="Link copied successfully!"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"> <span>Recent Update</span><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/tsne/">T-SNE 이해하기</a><li><a href="/posts/how_to_work_python/">CS50 - 파이썬이 소스 코드를 실행하는 과정과 원리</a><li><a href="/posts/OIDC/">IRSA의 원리를 파헤쳐보자 4 - OIDC</a><li><a href="/posts/OAuth/">IRSA의 원리를 파헤쳐보자 3 - OAuth2.0</a><li><a href="/posts/admission_webhook/">IRSA의 원리를 파헤쳐보자 1 - K8S Admission Webhook</a></ul></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"> <span class="pl-3 pt-2 mb-2">Contents</span><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/fasttext/"><div class="card-body"> <span class="timeago small" >Aug 16, 2020<i class="unloaded">2020-08-16T00:00:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Enriching Word Vectors with Subword Information</h3><div class="text-muted small"><p> Fasttext 논문(Enriching Word Vectors with Subword Information)을 리뷰해 보았습니다. 슬라이드쉐어에 동일한 내용을 업로드 하였으며, 슬라이드쉐어에서 보시려면 이곳을 확인해 주세요. 1   Introduction 2   General Model 3   Subwor...</p></div></div></a></div><div class="card"> <a href="/posts/nlp_transfer_learning_history/"><div class="card-body"> <span class="timeago small" >Mar 5, 2020<i class="unloaded">2020-03-05T00:00:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>NLP Transfer Learning History</h3><div class="text-muted small"><p> 김성현 연구원님의 T아카데미 토크ON세미나 ‘딥러닝 기반의 자연어 언어 모델 BERT’ 라는 세미나를 듣고 알게된 정보와 구글링을 통해 알게된 정보를 종합하여 관련 내용을 정리해 보고자 합니다. 1   From word embedding To pretrained language models 1.1   Traditional context-fr...</p></div></div></a></div><div class="card"> <a href="/posts/byte_pair-encoding/"><div class="card-body"> <span class="timeago small" >Jun 5, 2020<i class="unloaded">2020-06-05T00:00:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Byte Pair Encoding</h3><div class="text-muted small"><p> 최근 NLP에서 tokenizer로 많이 사용되고 있는 BPE에 대해서 간단하게 정리해 보겠습니다. 전체코드는 이곳에서 확인해 보실 수 있습니다. 1   Backgroud: Subword Segmentation subword segmentation(단어 분리, 단어 분절)이란, 하나의 단어(혹은 토큰)는 여러 개의 subword의 조합으로 이...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/Sieve_of_Eratosthenes/" class="btn btn-outline-primary" prompt="Older"><p>에라토스테네스의 체</p></a> <a href="/posts/deep_learning_is_robust_to_massive_label_noise/" class="btn btn-outline-primary" prompt="Newer"><p>Deep Learning is Robust to Massive Label Noise</p></a></div></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2025 <a href="https://github.com/gguguk">Gukwon Koo</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">Trending Tags</h4><a class="post-tag" href="/tags/python/">python</a> <a class="post-tag" href="/tags/retrospective/">retrospective</a> <a class="post-tag" href="/tags/k8s/">k8s</a> <a class="post-tag" href="/tags/kubernetes/">kubernetes</a> <a class="post-tag" href="/tags/nlp/">nlp</a> <a class="post-tag" href="/tags/statistics/">statistics</a> <a class="post-tag" href="/tags/irsa/">irsa</a> <a class="post-tag" href="/tags/ml/">ml</a> <a class="post-tag" href="/tags/mlops/">mlops</a> <a class="post-tag" href="/tags/paper/">paper</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="https://gguguk.github.io{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/lozad/dist/lozad.min.js,npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.16.1,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id=G-8EWVG7CHCY"></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'G-8EWVG7CHCY'); }); </script>

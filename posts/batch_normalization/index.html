<!DOCTYPE html><html lang="en" mode="light" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta http-equiv="Cache-Control" content="no-cache, no-store, must-revalidate"><meta http-equiv="Expires" content="0"><meta http-equiv="Pragma" content="no-cache"><meta name="day-prompt" content="days ago"><meta name="hour-prompt" content="hours ago"><meta name="minute-prompt" content="minutes ago"><meta name="justnow-prompt" content="just now"><meta name="generator" content="Jekyll v4.4.1" /><meta property="og:title" content="Batch Normalization - Accelerating Deep Network Training by Reducing Internal Covariate Shift" /><meta name="author" content="Gukwon Koo" /><meta property="og:locale" content="en" /><meta name="description" content="배치 정규화(batch normalization) 기법을 자주 활용했으나, 정확한 작동 원리에 대해서 알지 못했기 때문에 논문을 읽고 내용을 정리해 보았습니다. 친절한 논문은 아니어서 읽는데 꽤 시간이 걸렸습니다. 또한 관련 자료를 탐색 하던 중 배치 정규화 논문에서 주장하는 covariate shift에 대해 반박하는 논문도 있다는 정보를 보았는데요. 이 논문도 따로 정리해 보겠습니다. 먼저 논문의 내용을 요약하면 다음과 같습니다." /><meta property="og:description" content="배치 정규화(batch normalization) 기법을 자주 활용했으나, 정확한 작동 원리에 대해서 알지 못했기 때문에 논문을 읽고 내용을 정리해 보았습니다. 친절한 논문은 아니어서 읽는데 꽤 시간이 걸렸습니다. 또한 관련 자료를 탐색 하던 중 배치 정규화 논문에서 주장하는 covariate shift에 대해 반박하는 논문도 있다는 정보를 보았는데요. 이 논문도 따로 정리해 보겠습니다. 먼저 논문의 내용을 요약하면 다음과 같습니다." /><link rel="canonical" href="https://gguguk.github.io/posts/batch_normalization/" /><meta property="og:url" content="https://gguguk.github.io/posts/batch_normalization/" /><meta property="og:site_name" content="생각과 고민." /><meta property="og:type" content="article" /><meta property="article:published_time" content="2020-11-03T09:42:00+09:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Batch Normalization - Accelerating Deep Network Training by Reducing Internal Covariate Shift" /><meta name="twitter:site" content="@twitter_username" /><meta name="twitter:creator" content="@Gukwon Koo" /><meta name="google-site-verification" content="google_meta_tag_verification" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Gukwon Koo"},"dateModified":"2020-11-03T09:42:00+09:00","datePublished":"2020-11-03T09:42:00+09:00","description":"배치 정규화(batch normalization) 기법을 자주 활용했으나, 정확한 작동 원리에 대해서 알지 못했기 때문에 논문을 읽고 내용을 정리해 보았습니다. 친절한 논문은 아니어서 읽는데 꽤 시간이 걸렸습니다. 또한 관련 자료를 탐색 하던 중 배치 정규화 논문에서 주장하는 covariate shift에 대해 반박하는 논문도 있다는 정보를 보았는데요. 이 논문도 따로 정리해 보겠습니다. 먼저 논문의 내용을 요약하면 다음과 같습니다.","headline":"Batch Normalization - Accelerating Deep Network Training by Reducing Internal Covariate Shift","mainEntityOfPage":{"@type":"WebPage","@id":"https://gguguk.github.io/posts/batch_normalization/"},"url":"https://gguguk.github.io/posts/batch_normalization/"}</script><title>Batch Normalization - Accelerating Deep Network Training by Reducing Internal Covariate Shift | 생각과 고민.</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="생각과 고민."><meta name="application-name" content="생각과 고민."><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column align-items-end" lang="en"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="/assets/img/sample/bear.png" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">생각과 고민.</a></div><div class="site-subtitle font-italic">주니어 데이터 사이언티스트입니다.</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tags ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center"> <a href="https://github.com/gguguk" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github-alt"></i> </a> <a href="https://www.linkedin.com/in/%EA%B5%AD%EC%9B%90-%EA%B5%AC-32a9691a1/" aria-label="linkedin" target="_blank" rel="noopener"> <i class="fab fa-linkedin"></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>Batch Normalization - Accelerating Deep Network Training by Reducing Internal Covariate Shift</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>Batch Normalization - Accelerating Deep Network Training by Reducing Internal Covariate Shift</h1><div class="post-meta text-muted d-flex flex-column"><div> <span class="semi-bold"> Gukwon Koo </span> on <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="Tue, Nov 3, 2020, 9:42 AM +0900" >Nov 3, 2020<i class="unloaded">2020-11-03T09:42:00+09:00</i> </span></div><div> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="2553 words">14 min read</span></div></div><div class="post-content"><p>배치 정규화(batch normalization) 기법을 자주 활용했으나, 정확한 작동 원리에 대해서 알지 못했기 때문에 논문을 읽고 내용을 정리해 보았습니다. 친절한 논문은 아니어서 읽는데 꽤 시간이 걸렸습니다. 또한 관련 자료를 탐색 하던 중 배치 정규화 논문에서 주장하는 covariate shift에 대해 반박하는 논문도 있다는 정보를 보았는데요. 이 논문도 따로 정리해 보겠습니다. 먼저 논문의 내용을 요약하면 다음과 같습니다.</p><ul><li>뉴럴넷 학습을 방해하는 요소로 <strong>Internal Covariate Shift</strong> 현상을 지목합니다. 다시 말해 학습이 진행되는 동안 network parameter들이 변화하면서 network activation의 distribution이 변화하기 때문에 뉴럴넷의 학습이 느려진다는 것입니다.<li>배치 정규화는 각 레이어의 출력값을 평균 0, 분산 1의 표준정규분포로 변환하기 때문에 covariate shift 문제를 벗어날 수 있습니다.<li>배치 정규화를 적용하면 다음의 이점을 얻을 수 있습니다.<ul><li><strong>학습률을 크게</strong> 셋팅할 수 있기 때문에 배치 정규화를 적용하지 않았을 때 대비 수렴 속도가 빠릅니다. 따라서 학습률 감소(learning rate decay)를 기존 보다 빠르게 가져갈 수 있습니다.<li>randomness를 주기 때문에 regularization 효과가 있습니다. 따라서 <strong>드롭 아웃을 제거</strong> 할 수 있으며, 오버 피팅이 방지되는 효과가 있습니다.</ul></ul><p><br /></p><h2 id="1--introduction">1   Introduction</h2><p>각 레이어의 인풋은 모든 이전 레이어의 파라미터(저자는 가중치라는 표현 대신 파라미터라는 표현을 사용합니다)에 영향을 받습니다. 따라서 앞선 레이어 파라미터의 작은 변화는 네트워크의 층이 깊어질수록 증폭될 것입니다. 각 레이어들은 이전 레이어의 출력 분포의 변화에 맞춰서 계속 해서 학습을 이어나가야 하기 때문에 이전 레이어의 출력 분포의 변화는 네트워크의 학습을 방해합니다.</p><p>저자는 이러한 현상을 <strong>internal covariate shift</strong>라고 부릅니다. 만일 각 출력층의 분포를 일정하게 유지할 수 있다면 covariate shift 현상을 억제하여 각 레이어가 이전 레이어의 출력 분포의 변화에 재적응(readjust)할 필요가 없으므로 학습을 원할하게 진행시킬 수 있을 것입니다.</p><p>레이어 출력 분포를 일정하게 유지시키는 것은 <strong>gradient를 잘 흐를 수 있게 한다는 이점</strong>도 있습니다. 시그모이드 활성화 함수 \(g(x) = \cfrac {1} {1 + exp(-x)}\)를 생각해 보죠. \(\vert x \vert\) 값이 커지면 그때의 gradient는 거의 0에 수렴하게 됩니다. 따라서 vanish graident 문제는 학습을 느리게 하는 주요 원인 입니다.</p><p><img data-proofer-ignore data-src="https://upload.wikimedia.org/wikipedia/commons/8/88/Logistic-curve.svg" alt="시그모이드 함수" /><em>\(\vert x \vert\)값이 커지면 그때의 미분계수는 0에 가까워짐을 알 수 있습니다.</em></p><p>따라서 저자는 <strong>배치 정규화 기법(batch normalization)</strong>의 도입이 네트워크의 각 레이어의 출력 분포를 일정하게 만들어서 네트워크를 빠르게 수렴시킬 수 있다고 주장합니다. 배치 정규화는 네트워크 각 레이어의 출력 분포를 일정하게 유지하여 internal covariate shift 문제를 완화시켜 딥 뉴럴넷의 학습을 가속화시킵니다. 배치 정규화의 자세한 과정은 뒤에서 다시 언급하도록 하겠습니다. 이제 좀더 자세히 알아 보도록 하죠 👊</p><p><br /></p><h2 id="2--towards-reducing-internal-covariate-shift">2   Towards Reducing Internal Covariate Shift</h2><p>본 섹션에서 저자는 covariate shift를 줄이기 위해 필요한</p><p>먼저 internal covariate shift를 정확히 정의해야하겠습니다. <strong>internal covariate shift</strong>는 학습이 진행되는 동안 네트워크의 파라미터가 지속적으로 갱신됨에 따라 발생하는 네트워크 각 레이어의 출력 분포의 변화를 뜻합니다. 뉴럴넷 학습 과정에서 whitening을 통해 분포를 고정(fixing)하는 것은 학습의 수렴을 빠르게 한다는 것이 기존 연구에 의해 알려져 있습니다([<a href="https://d1wqtxts1xzle7.cloudfront.net/30766372/lecun-98b.pdf?1362337683=&amp;response-content-disposition=inline%3B+filename%3DEfficient_backprop.pdf&amp;Expires=1604822929&amp;Signature=X01Sh-pxL0yZlus-RReXPglX7lHY-oRnheycmne3DWuOoZB0oTZieX2W2TJn4mfkoLWDCerpKm-zC973c5JpR1hRVaZtkMnjawsj88zgpnkTmHD406QHsXjLjt0mzN~6CMGRLUDACtjqBTRs9wdrtQFqi6mdOXDAAUZ91iqs8KVTnZA9-Q22uqH7TYtOWzk5zUMr3910WJ7JAJRinV55q4xlXwNvkCXv6P~YJJL6W7rdw7TuwSXQxKlHxKc-Am8L7eRcZ5~qH-X9NUiqFHmBohW3s~z-Nc-dwSYAn4WPrUq3feiQFwSY~AgnHLc~RI8ln-eOscAaq~dRUghWCVSGlA__&amp;Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA">2</a>]). activation을 whitening 하는 방법은 다양합니다. 하지만 중요한 점은 whitening 과정이 반드시 gradient descent 과정에 포함되어야 한다는 것입니다. 그렇지 않다면 어떻게 될까요? 예를 들어 input \(u\)와 learnable bias \(b\)를 더한후 normalization을 진행하여 \(\hat x\)을 출력하는 간단한 네트워크를 생각해봅시다.</p>\[\hat x = x - E[x],\] \[\text {where} \space x=u+b, \space X=\{x_{1 ...N}\} \space \text{is the set of values of} \space x \space \text{over the training set}, \space \text{and} \space E[x] = \frac{1}{N} \textstyle \sum_{i=1}^{N}x_{i}\]<p>만일 gradient descent 과정에서 \(E[x]\)의 \(b\)에 대한 의존성을 고려하지 않고 계산 된다면 어떻게 될까요? \(E(X)\) 방향으로 gradient를 흘려 보내 않을 때의 상황을 생각하면 직관적으로 이해가 됩니다. 아래의 그림에서 ❌ 부분에 gradient가 흐르지 않는다고 생각해보세요.</p><ul><li><p>\(\Delta b\)는 \(\frac{\partial l}{\partial \hat x}\)에 비례하게 됩니다.</p><li><p>epoch가 한번 돌면 \(b\)는 \(b+\Delta{b}\)로 업데이트 됩니다.</p><li><p>따라서 새로운 \(\hat{x}\)는 아래와 같이 계산 됩니다.</p>\[\begin{aligned} \hat{x} &amp;= u+(b+\Delta{b}) - E[u+(b+\Delta{b})] \\ &amp;= u+b+\Delta{b}-E[u+b]-\Delta{b} \\ &amp;= u+b - E[u+b] \\ &amp;= x-E[x] \end{aligned}\]</ul><p>분명히 \(b\)를 업데이트 했는데도 불구하고 \(\hat{x}\)의 값이 업데이트 전과 동일합니다. 이렇게 되면 파라미터 \(b\)가 발산해 버리게 됩니다. 왜냐하면 \(b\)는 \(\Delta{b}\)만큼 계속 해서 그 값이 커지는데, \(\hat{x}\) 값은 학습을 아무리 진행해도 특정 값으로 고정되기 때문입니다. \(\Delta{b}\)는 \(\frac{\partial l}{\partial \hat{x}}\)에 비례하는데, 계산 그래프 상에서 \(b\)를 제외한 모든 값이 고정적이기 때문에 \(\Delta{b}\)는 일정하게 계속 증가되어 결국 발산해버리는 것이죠. 저자는 초기 실험에서 이러한 상황을 경험적으로 확인하였으며, loss에 대한 gradient를 구할 때 normalization 과정도 함께 고려함으로써 문제를 해결했다고 합니다.</p><p><img data-proofer-ignore data-src="/assets/img/post_img/bn_1.png" alt="" /><em>간단한 레이어의 계산 그래프. \(E(x)\)의 \(b\)에 대한 의존성을 고려하지 않으면 학습 과정상에 문제가 발생하게 됩니다.</em></p><p>이제는 사고를 한차원 확장하여 \(x\)를 vector로 생각하고 \(X\)를 이러한 vector들의 set으로 생각해봅시다. \(x\)는 \(u\)와 \(b\)를 곱한 값이라는 점을 주의해주세요. 이러한 셋팅에서 \(\hat{x}\)를 구하는 과정을 다음과 같이 단순할 수 있습니다.</p>\[\hat{x} = Norm(x, X)\]<p>\(\hat{x}\)는 개별 example인 \(x\)뿐만 아니라 전체 데이터셋 \(X\)에도 의존한다는 것을 알 수 있습니다. 또한 역전파를 위해서는 다음과 같은 자코비안 행렬을 계산해야 합니다.</p>\[\cfrac{\partial \text{Norm}(x, X)}{\partial x} \space \text{and} \space \cfrac{\text{Norm}(x, X)}{\partial X}\]<p>위 식에서 두번째 term을 무시하게 되면 앞서 언급한 \(b\)의 발산 문제가 똑같이 발생하게 됩니다. 왜 두번째 term과 앞서 언급한 현상이 관련이 있다는 것일까요? 충분히 고민하다가 내린 개인적인 생각을 덧붙여보겠습니다. 전체 데이터셋이 2개인 상황을 가정해보면 normalization 계산 그래프는 아래의 figure처럼 표현 할 수 있습니다. \(b\)가 발산하는 문제는 \(E(x)\)쪽 경로의 gradient를 고려하지 않았을 때 발생한다는 점을 앞서 언급하였습니다. 그런데 \(E(x)\)를 구하는 과정에서 전체 데이터셋 \(X\)가 필요하게 됩니다. 데이터셋이 2개인 상황을 가정하였으므로 \(X=\{x_{1}, x_{2}\}\)라고 생각할 수 있겠네요. 결국 위 식에서 말하는 전체 데이터 셋에 대한 미분값은 \(E(x)\) 쪽 경로로 흘러가는 gradient라고 생각할 수 있습니다.</p><p><img data-proofer-ignore data-src="/assets/img/post_img/bn_2.png" alt="" width="600" /></p><p>다시 논점으로 돌아와서, 이러한 셋팅에서 normalization을 진행하기 위해서는 다음과 같은 공분산 행렬과 그것의 inverse square root에 대한 연산이 필요하며 해당 연산들에 대한 미분값 역시 필요하게 됩니다. normalization 과정에 엄청난 연산이 요구되는 것이죠.</p>\[\text{Cov}[x] = E_{x \in X}[xx^T] - E[x]E[x]^T \space \text{and} \space \text{Cov}[x]^{-1/2}(x-E[x])\]<p>따라서 저자는 1) 미분 가능하면서(gradient descent 과정에 포함시키기 위해) 2) 전체 트레이닝 셋을 고려하지 않는 normalization기법을 고안하게 됩니다. 바로 <strong>배치 정규화</strong> 기법이죠!</p><p><br /></p><h2 id="3-normalization-via-mini-batch-statistics">3   Normalization via MIni-Batch Statistics</h2><p>앞서 언급했다시피 full whitening은 매우 계산량이 높고, 모든 곳에서 미분이 가능하지 않다는 단점이 있습니다. 따라서 저자는 각각의 feature들을 독립적으로 zero mean and unit variance로 만드는 방법을 제안합니다. 이러한 normalization 방식은 feature 간 decorrelated가 되지 않았을 때도 모델의 수렴 속도를 빠르게 만든다는 것이 기존 연구에서 입증되었습니다(<a href="https://d1wqtxts1xzle7.cloudfront.net/30766372/lecun-98b.pdf?1362337683=&amp;response-content-disposition=inline%3B+filename%3DEfficient_backprop.pdf&amp;Expires=1605169879&amp;Signature=OP2n7kjEKuPCVkOAvv~~FxH4hXP2N4bvEFr0exZVdejvs0mB29QgJFZnkgIqpGJ0La4mkhTAsjE-F59RhiC12-BLqQ73K0M~bv5~ogwwDR459C36tqcp38D0mN0~W1MDAgm4g7xfDqDYlzeHV5fXCiWkYHuncwmIji1irzbxrwy~I5LDSL-FSfEMMXTeE05C4~92BIpJr55uq0drgfYuSW36NPqfKgRjRGIxQ3KelBSa5ouGevZLlElzq4Y5wH14IFWzBs84E9KTConCcM1vkjN-ZNzKLI7PJ9eQBFSZ9FMlSzC7wH~NXdXbypcgHzUZUme9dV8luF2TEGMEtH6-sQ__&amp;Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA">LeCun et al., 1998</a>).</p><p><br /></p><h2 id="4-experiments">4   Experiments</h2><p><br /></p><h2 id="5--conclusion">5   Conclusion</h2><p><br /></p><h2 id="reference">Reference</h2><h3 id="papers">Papers</h3><ul><li><p><a href="https://arxiv.org/abs/1502.03167">Ioffe, S., &amp; Szegedy, C. (2015). Batch normalization: Accelerating deep network training by reducing internal covariate shift. <em>arXiv preprint arXiv:1502.03167</em>.</a></p><li><p><a href="https://d1wqtxts1xzle7.cloudfront.net/30766372/lecun-98b.pdf?1362337683=&amp;response-content-disposition=inline%3B+filename%3DEfficient_backprop.pdf&amp;Expires=1604822929&amp;Signature=X01Sh-pxL0yZlus-RReXPglX7lHY-oRnheycmne3DWuOoZB0oTZieX2W2TJn4mfkoLWDCerpKm-zC973c5JpR1hRVaZtkMnjawsj88zgpnkTmHD406QHsXjLjt0mzN~6CMGRLUDACtjqBTRs9wdrtQFqi6mdOXDAAUZ91iqs8KVTnZA9-Q22uqH7TYtOWzk5zUMr3910WJ7JAJRinV55q4xlXwNvkCXv6P~YJJL6W7rdw7TuwSXQxKlHxKc-Am8L7eRcZ5~qH-X9NUiqFHmBohW3s~z-Nc-dwSYAn4WPrUq3feiQFwSY~AgnHLc~RI8ln-eOscAaq~dRUghWCVSGlA__&amp;Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA">LeCun, Y. A., Bottou, L., Orr, G. B., &amp; Müller, K. R. (2012). Efficient backprop. In <em>Neural networks: Tricks of the trade</em> (pp. 9-48). Springer, Berlin, Heidelberg.</a></p></ul><p><br /></p><h3 id="articles">Articles</h3><ul><li><p><a href="https://goodjian.tistory.com/entry/%EB%B0%B0%EC%B9%98-%EC%A0%95%EA%B7%9C%ED%99%94-%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-Batch-normalization">배치 정규화 논문 리뷰 (Batch normalization) - Slow walking man</a>https://arxiv.org/abs/1404.5997)</p><li><p><a href="https://hcnoh.github.io/2018-11-27-batch-normalization">[Deep Learning] Batch Normalization 개념 정리</a></p></ul></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/ml/'>ML</a>, <a href='/categories/paper-review/'>Paper Review</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/ml/" class="post-tag no-text-decoration" >ml</a> <a href="/tags/dl/" class="post-tag no-text-decoration" >dl</a> <a href="/tags/paper/" class="post-tag no-text-decoration" >paper</a> <a href="/tags/review/" class="post-tag no-text-decoration" >review</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=Batch Normalization - Accelerating Deep Network Training by Reducing Internal Covariate Shift - 생각과 고민.&url=https://gguguk.github.io/posts/batch_normalization/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=Batch Normalization - Accelerating Deep Network Training by Reducing Internal Covariate Shift - 생각과 고민.&u=https://gguguk.github.io/posts/batch_normalization/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=Batch Normalization - Accelerating Deep Network Training by Reducing Internal Covariate Shift - 생각과 고민.&url=https://gguguk.github.io/posts/batch_normalization/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" title-succeed="Link copied successfully!"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"> <span>Recent Update</span><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/tsne/">T-SNE 이해하기</a><li><a href="/posts/how_to_work_python/">CS50 - 파이썬이 소스 코드를 실행하는 과정과 원리</a><li><a href="/posts/OIDC/">IRSA의 원리를 파헤쳐보자 4 - OIDC</a><li><a href="/posts/OAuth/">IRSA의 원리를 파헤쳐보자 3 - OAuth2.0</a><li><a href="/posts/admission_webhook/">IRSA의 원리를 파헤쳐보자 1 - K8S Admission Webhook</a></ul></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"> <span class="pl-3 pt-2 mb-2">Contents</span><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/deep_learning_is_robust_to_massive_label_noise/"><div class="card-body"> <span class="timeago small" >Oct 20, 2020<i class="unloaded">2020-10-20T10:00:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Deep Learning is Robust to Massive Label Noise</h3><div class="text-muted small"><p> 현업의 데이터를 다루다보면 데이터에 noisy label이 많이 발생합니다. 가장 좋은 점은 일일이 수작업으로 data cleaning을 하는 것이 좋겠지만, 데이터의 양이 늘어남에 따라 라벨링을 교정하는 것 자체가 일이 되는 경우가 발생하게 됩니다. 이와 관련된 고민을 하던 중 noisy label이 모델에 끼치는 영향력을 분석한 논문을 발견하여 읽...</p></div></div></a></div><div class="card"> <a href="/posts/fasttext/"><div class="card-body"> <span class="timeago small" >Aug 16, 2020<i class="unloaded">2020-08-16T00:00:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Enriching Word Vectors with Subword Information</h3><div class="text-muted small"><p> Fasttext 논문(Enriching Word Vectors with Subword Information)을 리뷰해 보았습니다. 슬라이드쉐어에 동일한 내용을 업로드 하였으며, 슬라이드쉐어에서 보시려면 이곳을 확인해 주세요. 1   Introduction 2   General Model 3   Subwor...</p></div></div></a></div><div class="card"> <a href="/posts/random_forest/"><div class="card-body"> <span class="timeago small" >Jul 15, 2020<i class="unloaded">2020-07-15T00:00:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Random Forest</h3><div class="text-muted small"><p> 랜덤 포레스트를 이해하기 위해 필요한 bagging과 관련된 확률 이론을 정리해보겠습니다. 1   Bagging Bootstrap data point가 n개 있을 때, bootstrap은 n개의 크기를 가진 표본을 복원 추출하는 방법을 말합니다. Bootstrap을 통해 추출된 크기가 \(n\)인 표본은 \(X=\{X_{1}, X_{2},...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/statistics_110/" class="btn btn-outline-primary" prompt="Older"><p>하버드 확률론 기초 강의(Statistics 110)을 완강하고 나서 느낀점, 앞으로의 계획</p></a> <a href="/posts/discrete_distributions/" class="btn btn-outline-primary" prompt="Newer"><p>이산형 확률 분포 정리</p></a></div></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2025 <a href="https://github.com/gguguk">Gukwon Koo</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">Trending Tags</h4><a class="post-tag" href="/tags/python/">python</a> <a class="post-tag" href="/tags/retrospective/">retrospective</a> <a class="post-tag" href="/tags/k8s/">k8s</a> <a class="post-tag" href="/tags/kubernetes/">kubernetes</a> <a class="post-tag" href="/tags/nlp/">nlp</a> <a class="post-tag" href="/tags/statistics/">statistics</a> <a class="post-tag" href="/tags/irsa/">irsa</a> <a class="post-tag" href="/tags/ml/">ml</a> <a class="post-tag" href="/tags/mlops/">mlops</a> <a class="post-tag" href="/tags/paper/">paper</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="https://gguguk.github.io{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/lozad/dist/lozad.min.js,npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.16.1,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id=G-8EWVG7CHCY"></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'G-8EWVG7CHCY'); }); </script>

<!DOCTYPE html><html lang="en" mode="light" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta http-equiv="Cache-Control" content="no-cache, no-store, must-revalidate"><meta http-equiv="Expires" content="0"><meta http-equiv="Pragma" content="no-cache"><meta name="day-prompt" content="days ago"><meta name="hour-prompt" content="hours ago"><meta name="minute-prompt" content="minutes ago"><meta name="justnow-prompt" content="just now"><meta name="generator" content="Jekyll v4.4.1" /><meta property="og:title" content="Random Forest" /><meta name="author" content="Gukwon Koo" /><meta property="og:locale" content="en" /><meta name="description" content="랜덤 포레스트를 이해하기 위해 필요한 bagging과 관련된 확률 이론을 정리해보겠습니다." /><meta property="og:description" content="랜덤 포레스트를 이해하기 위해 필요한 bagging과 관련된 확률 이론을 정리해보겠습니다." /><link rel="canonical" href="https://gguguk.github.io/posts/random_forest/" /><meta property="og:url" content="https://gguguk.github.io/posts/random_forest/" /><meta property="og:site_name" content="생각과 고민." /><meta property="og:type" content="article" /><meta property="article:published_time" content="2020-07-15T00:00:00+09:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Random Forest" /><meta name="twitter:site" content="@twitter_username" /><meta name="twitter:creator" content="@Gukwon Koo" /><meta name="google-site-verification" content="google_meta_tag_verification" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Gukwon Koo"},"dateModified":"2020-07-15T00:00:00+09:00","datePublished":"2020-07-15T00:00:00+09:00","description":"랜덤 포레스트를 이해하기 위해 필요한 bagging과 관련된 확률 이론을 정리해보겠습니다.","headline":"Random Forest","mainEntityOfPage":{"@type":"WebPage","@id":"https://gguguk.github.io/posts/random_forest/"},"url":"https://gguguk.github.io/posts/random_forest/"}</script><title>Random Forest | 생각과 고민.</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="생각과 고민."><meta name="application-name" content="생각과 고민."><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column align-items-end" lang="en"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="/assets/img/sample/bear.png" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">생각과 고민.</a></div><div class="site-subtitle font-italic">주니어 데이터 사이언티스트입니다.</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tags ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center"> <a href="https://github.com/gguguk" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github-alt"></i> </a> <a href="https://www.linkedin.com/in/%EA%B5%AD%EC%9B%90-%EA%B5%AC-32a9691a1/" aria-label="linkedin" target="_blank" rel="noopener"> <i class="fab fa-linkedin"></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>Random Forest</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>Random Forest</h1><div class="post-meta text-muted d-flex flex-column"><div> <span class="semi-bold"> Gukwon Koo </span> on <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="Wed, Jul 15, 2020, 12:00 AM +0900" >Jul 15, 2020<i class="unloaded">2020-07-15T00:00:00+09:00</i> </span></div><div> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="1929 words">10 min read</span></div></div><div class="post-content"><blockquote><p>랜덤 포레스트를 이해하기 위해 필요한 bagging과 관련된 확률 이론을 정리해보겠습니다.</p></blockquote><h2 id="1--bagging">1   Bagging</h2><h3 id="bootstrap">Bootstrap</h3><p>data point가 n개 있을 때, bootstrap은 n개의 크기를 가진 표본을 복원 추출하는 방법을 말합니다. Bootstrap을 통해 추출된 크기가 \(n\)인 표본은 \(X=\{X_{1}, X_{2}, ..., X_{n}\}\)으로 표기 할 수 있습니다.</p><p>중요한 점은 \(n\)개의 표본단위 \(X_{1}, X_{2}, ..., X_{n}\)를 <strong>정해진 어떤 값</strong>으로 생각하면 안 된다는 것입니다. \(X_{n}\)을 \(n\)번째에 추출된 어떤 값으로 생각해야하며, 표본 \(X\)에 속하는 각각의 표본 단위는 모두 확률 변수(random variable)입니다. 또한 각 표본 단위 \(X_{n}\)은 모집단 \(X\)와 같은 확률 분포입니다.</p><p>이를 <strong>동일 분포(identical distribution)</strong>, <strong>독립 분포(independent distribution)</strong>인 확률 변수(random variables)라고 말합니다.</p><ul><li>동일 분포: 각 표본 단위들이 동일한 분포임을 뜻합니다. 모집단에서 추출한 표본 단위는 모집단의 분포를 그대로 가지고 있으므로, 하나의 표본에 존재하는 모든 표본 단위는 동일한 분포를 가지고 있습니다.<li>독립 분포: 각 표본 단위가 추출되는 것이 다른 표본 단위가 추출되는 확률에 영향을 끼치지 않는 특성을 뜻합니다. 이는 표본 추출이 ‘무작위 복원 추출’을 전제하기 때문입니다. 따라서 각 표본 단위들이 추출된 확률은 모두 동일합니다. 즉 \(P(X_{n}) = 1/n\)입니다.</ul><p>축약어로는 iid(identical and independent distribution)라고 하며, 다음과 같이 표기할 수 있습니다(\(X\)는 모집단의 확률 변수를 나타냄)</p>\[\{X_{1}, X_{2}, ..., X_{n}\} \sim i.i.d \space f_{X}(x)\]<p>이를 다른 말고 확률 표본(random sample)이라고도 합니다. 확률 표본이란 모집단에 속하는 모든 단위가 표본에 추출될 확률이 모두 동일하도록 추출되는 표본을 말합니다. 즉 \(X_{1}, X_{2}, ..., X_{n}\)의 표본에서 아래의 수식이 성립됩니다.</p><p>\(P(X_{1}) = P(X_{2}) = ... = P(X_{n})\) \(P(X_{1}, X_{2}, ..., X_{n}) = P(X_{1})P(X_{2})...P(X_{n})\)</p><h3 id="11--whats-the-bagging">1.1   What’s the Bagging?</h3><p>Bagging은 Bootstrap aggregating의 약자 Bootstrap은 모집단에서 복원 추출한 sample들을 뜻합니다. 배깅 방법은 uniform 확률 분포에서 독립적이면서 랜덤 복원 추출로 선택된 data들을 이용합니다.</p><p>학습 데이터가 \(X=x_{1}, ..., x_{n}\) 대응 되는 반응 변수가 \(Y=y_{1},...,y_{n}\) 일때, bagging은 \(B\)번 만큼 반복 시행됩니다.</p><p>For \(b=1,...,B\):</p><ol><li>Sample, with replacement, $n$ training examples from \(X, Y\); call these \(X_{b}, Y_{b}\)<li>Train a classification or regression tree \(f_{b}\) on \(X_{b}, Y_{b}\)</ol><p>학습이 완료된 후 unseen sampels \(x'\)에 대한 prediction은 모든 개별적인 tree의 결과를 averaging 하여 나타냅니다.</p>\[\hat{f} = \cfrac{1}{b}\sum_{b=1}^{B}f_{b}(x')\]<p>bagging은 identical distribution 조건을 만족하지만, independence distribution 조건을 만족시키지 못합니다. 예를들어, tree estimator를 만들 때, 어느 tree에서나 중요하게 사용되는 feature가 있다고 생각해봅시다. 사람의 성별을 예측하는 문제에서는 ‘키’와 같은 변수가 되겠습니다. 이 경우 어느 tree에서나 ‘키’ 변수는 첫번째 split 기준으로 선택될 것입니다. 그 결과 대부분의 tree의 prediction 결과는 비슷해질 것입니다. 다시 말해 각 tree의 prediction 값이 correlated(not independent) 됩니다.</p><p>이를 볼 때, bagging은 iid 확률 변수들의 sequence(bootstrap samples)를 받아 tree estimator를 통해 id 확률 변수로 변환하는 과정이라고 생각해볼 수 있습니다.</p><h3 id="12--advantages-of-bagging">1.2   Advantages of Bagging</h3><p>bagging은 모델의 variance을 줄여서(bias는 상승시키지 않으면서) 모델의 성능을 끌어 올릴 수 있습니다. 이는 하나의 tree의 분산이 높아 학습 데이터의 이상치에 지나치게 sensitive 하더라도, 다수의 tree의 결과를 averaging 하면, 이상치에 강건(variance 낮음)한 모델을 만들 수 있다는 것을 의미합니다. <strong>그러나 이는 각 tree가 correlated 되지 않을 것을 전제로 합니다..</strong></p><p>Bagging succeeds in smoothing out this variance and hence reducing the test error.Bagging can dramatically reduce the variance of unstable procedures like trees, leading to improved prediction. A simple argument shows why bagging helps under squared-error loss, in short because averaging reduces variance and leaves bias unchanged.</p><h3 id="13--why-bagging-reduce-variance">1.3   Why bagging reduce variance??</h3><p>bootstrapping method is better when the training samples are sparase, and the subspace method is better when the classes are compact and the boundaries are smooth.</p><h2 id="2--random-subspace-method">2   Random subspace method</h2><h3 id="21--whats-the-random-subspace-method">2.1   What’s the Random subspace method?</h3><p>Random subspace method(이하 RSM)는 bagging과 유사하지만, features(attributes, predictors, independent variables)를 랜덤 복원 추출한다는 점에서 차이가 있습니다.</p><p>RSM는 feature bagging 또는 attribute bagging이라고도 불립니다. RSM은 앙상블 방법에 활용된 각 classifiers(estimators) 간의 correlation을 낮추는 역할을 합니다.</p><p>In the random subspace method, in each pass all the training points are projected onto a randomly chosen coordinate subspace in which a classifier is derived. The combined classifier is called a random forest</p><h2 id="2--from-bagging-to-random-forests">2   From Bagging to Random Forests</h2><h3 id="21--advatages-of-random-forest-over-single-tree">2.1   Advatages of random forest over single tree</h3><ul><li>bagging: it is better when the training samples are sparse<li>Random subspace method: it is better when the classes are compact and the boundaries are smooth.</ul><p>The subsampling method is preferable when the training set is very sparse relative to dimensionality, especially when coupled with a close-to-vanishing maximum Fisher’s ratio (0.3 or below), and when the class boundary is highly nonlinear.</p><ul><li><p><strong>Same Bias</strong>, but with <strong>lower variance</strong> over single decision Tree</p><li><p>decorrelate trees: Use a random subset of features in each step of growing each tree.</p><li><p>train dataset의 feature가 1개만 존재한다면, random forest는 사실상 bagging 알고리즘과 같다(random subspace method를 사용할 수 없기 때문)</p></ul><h2 id="3--appendix">3   Appendix</h2><ul><li><a href="https://stats.stackexchange.com/questions/302900/what-is-bayes-error-in-machine-learning">bayes error</a>: 이론적으로 달성할 수 있는 최소의 오차(irreducible error, 더 이상 줄일 수 없는 오차)</ul><h3 id="31--추정량estimator과-추정치estimate">3.1   추정량(estimator)과 추정치(estimate)</h3><blockquote><p>대학생들의 한 달 평균 용돈을 알기 위하여 100명의 대학생을 단순 무작위로 추출하여 조사한 결과, 표본 평균 \(\overline{X}=30\)만원 이었습니다. 따라서 모집단의 모수 \(\theta=\mu\)를 30만원 일것이라고 추정 하였습니다.</p></blockquote><p>이 때 표본 통계량 \(\hat{\theta}\)를 구하는 공식 \(\overline{X}=\cfrac{1}{n}\sum_{i=1}^{n}X_{i}\)는 추정량(estimator)라고 하고, 이에 따른 구체적인 수치 \(\bar{x}=30\)만원은 추정치(estimate)가 됩니다. 정리 하면 아래와 같습니다.</p><ul><li>추정량(estimator): 표본에 포함된 자료를 이용하여 추정치를 계산할 수 있는 법칙, 공식 또는 알고리즘<li>추정치(estimate): 추정량을 이용하여 계산된 구체적인 수치값</ul><h3 id="32--불편성bias">3.2   불편성(bias)</h3><p>아래 공식을 만족하는 추정량을 불편 추정량(unbiased estimator)라고 합니다.</p>\[E(\hat{\theta}) = \theta\]<p>참고로 표본의 통계치들은 <strong>확률 변수</strong>임을 반드시 기억해야 하겠습니다. 모수에서 크기가 \(n\)인 표본을 뽑게되면, 뽑을 때마다 표본의 통계치들을 달라질 것입니다. 따라서 표본 통계량 \(\hat{\theta}\)가 확률 변수이고 그에 상응하는 표본 분포(sample distribution)을 가지므로, 기대값을 구할 수 있는 것입니다.</p><h2 id="reference">Reference</h2><ul><li><a href="https://en.wikipedia.org/wiki/Random_forest">Random forest</a><li><a href="https://en.wikipedia.org/wiki/Random_subspace_method">Random subspace method</a><li><a href="https://link.springer.com/article/10.1007/s100440200009">A Data Complexity Analysis of Comparative Advantages of Decision Forest Constructors</a><li><a href="https://towardsdatascience.com/understanding-the-effect-of-bagging-on-variance-and-bias-visually-6131e6ff1385">Understanding the Effect of Bagging on Variance and Bias visually</a><li><a href="http://www.aistudy.co.kr/math/estimate_lee.htm">좋은 추정량(estimator)의 조건 - unbiased, efficiency, consistency</a><li><a href="http://www.sigmapress.co.kr/shop/shop_image/g93522_1404884177.pdf">모집단 평균의 추정</a><li><a href="http://blog.daum.net/cefotaxime/24">확률표본, 통계량, 모수, 표본평균의 분포</a><li><a href="https://stats.stackexchange.com/questions/89036/why-the-trees-generated-via-bagging-are-identically-distributed?rq=1">Why the trees generated via bagging are identically distributed?</a></ul></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/ml/'>ML</a>, <a href='/categories/statistical-learning/'>Statistical Learning</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/random-forest/" class="post-tag no-text-decoration" >random forest</a> <a href="/tags/ml/" class="post-tag no-text-decoration" >ml</a> <a href="/tags/statistical-learning/" class="post-tag no-text-decoration" >statistical learning</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=Random Forest - 생각과 고민.&url=https://gguguk.github.io/posts/random_forest/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=Random Forest - 생각과 고민.&u=https://gguguk.github.io/posts/random_forest/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=Random Forest - 생각과 고민.&url=https://gguguk.github.io/posts/random_forest/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" title-succeed="Link copied successfully!"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"> <span>Recent Update</span><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/tsne/">T-SNE 이해하기</a><li><a href="/posts/how_to_work_python/">CS50 - 파이썬이 소스 코드를 실행하는 과정과 원리</a><li><a href="/posts/OIDC/">IRSA의 원리를 파헤쳐보자 4 - OIDC</a><li><a href="/posts/OAuth/">IRSA의 원리를 파헤쳐보자 3 - OAuth2.0</a><li><a href="/posts/admission_webhook/">IRSA의 원리를 파헤쳐보자 1 - K8S Admission Webhook</a></ul></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"> <span class="pl-3 pt-2 mb-2">Contents</span><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/deep_learning_is_robust_to_massive_label_noise/"><div class="card-body"> <span class="timeago small" >Oct 20, 2020<i class="unloaded">2020-10-20T10:00:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Deep Learning is Robust to Massive Label Noise</h3><div class="text-muted small"><p> 현업의 데이터를 다루다보면 데이터에 noisy label이 많이 발생합니다. 가장 좋은 점은 일일이 수작업으로 data cleaning을 하는 것이 좋겠지만, 데이터의 양이 늘어남에 따라 라벨링을 교정하는 것 자체가 일이 되는 경우가 발생하게 됩니다. 이와 관련된 고민을 하던 중 noisy label이 모델에 끼치는 영향력을 분석한 논문을 발견하여 읽...</p></div></div></a></div><div class="card"> <a href="/posts/batch_normalization/"><div class="card-body"> <span class="timeago small" >Nov 3, 2020<i class="unloaded">2020-11-03T09:42:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Batch Normalization - Accelerating Deep Network Training by Reducing Internal Covariate Shift</h3><div class="text-muted small"><p> 배치 정규화(batch normalization) 기법을 자주 활용했으나, 정확한 작동 원리에 대해서 알지 못했기 때문에 논문을 읽고 내용을 정리해 보았습니다. 친절한 논문은 아니어서 읽는데 꽤 시간이 걸렸습니다. 또한 관련 자료를 탐색 하던 중 배치 정규화 논문에서 주장하는 covariate shift에 대해 반박하는 논문도 있다는 정보를 보았는데요...</p></div></div></a></div><div class="card"> <a href="/posts/nlp_transfer_learning_history/"><div class="card-body"> <span class="timeago small" >Mar 5, 2020<i class="unloaded">2020-03-05T00:00:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>NLP Transfer Learning History</h3><div class="text-muted small"><p> 김성현 연구원님의 T아카데미 토크ON세미나 ‘딥러닝 기반의 자연어 언어 모델 BERT’ 라는 세미나를 듣고 알게된 정보와 구글링을 통해 알게된 정보를 종합하여 관련 내용을 정리해 보고자 합니다. 1   From word embedding To pretrained language models 1.1   Traditional context-fr...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/probability_notation/" class="btn btn-outline-primary" prompt="Older"><p>Probability Notation</p></a> <a href="/posts/taylor_series/" class="btn btn-outline-primary" prompt="Newer"><p>Taylor Series</p></a></div></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2025 <a href="https://github.com/gguguk">Gukwon Koo</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">Trending Tags</h4><a class="post-tag" href="/tags/python/">python</a> <a class="post-tag" href="/tags/retrospective/">retrospective</a> <a class="post-tag" href="/tags/k8s/">k8s</a> <a class="post-tag" href="/tags/kubernetes/">kubernetes</a> <a class="post-tag" href="/tags/nlp/">nlp</a> <a class="post-tag" href="/tags/statistics/">statistics</a> <a class="post-tag" href="/tags/irsa/">irsa</a> <a class="post-tag" href="/tags/ml/">ml</a> <a class="post-tag" href="/tags/mlops/">mlops</a> <a class="post-tag" href="/tags/paper/">paper</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="https://gguguk.github.io{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/lozad/dist/lozad.min.js,npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.16.1,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id=G-8EWVG7CHCY"></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'G-8EWVG7CHCY'); }); </script>

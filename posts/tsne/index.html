<!DOCTYPE html><html lang="en" mode="light" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta http-equiv="Cache-Control" content="no-cache, no-store, must-revalidate"><meta http-equiv="Expires" content="0"><meta http-equiv="Pragma" content="no-cache"><meta name="day-prompt" content="days ago"><meta name="hour-prompt" content="hours ago"><meta name="minute-prompt" content="minutes ago"><meta name="justnow-prompt" content="just now"><meta name="generator" content="Jekyll v4.4.1" /><meta property="og:title" content="T-SNE 이해하기" /><meta name="author" content="Gukwon Koo" /><meta property="og:locale" content="en" /><meta name="description" content="T-SNE는 고차원의 임베딩을 저차원으로 변환하고 시각화화여 임베딩을 품질들 대략적으로 파악하는 것을 돕는 대표적인 차원 축소 알고리즘입니다. 실제로 아직도 현업에서도 많이 사용되는 알고리즘이고, 여러 논문에서도 임베딩의 질을 시각적으로 보여주기 위한 방법으로 많이 활용되고 있습니다." /><meta property="og:description" content="T-SNE는 고차원의 임베딩을 저차원으로 변환하고 시각화화여 임베딩을 품질들 대략적으로 파악하는 것을 돕는 대표적인 차원 축소 알고리즘입니다. 실제로 아직도 현업에서도 많이 사용되는 알고리즘이고, 여러 논문에서도 임베딩의 질을 시각적으로 보여주기 위한 방법으로 많이 활용되고 있습니다." /><link rel="canonical" href="https://gguguk.github.io/posts/tsne/" /><meta property="og:url" content="https://gguguk.github.io/posts/tsne/" /><meta property="og:site_name" content="생각과 고민." /><meta property="og:type" content="article" /><meta property="article:published_time" content="2025-08-30T14:52:00+09:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="T-SNE 이해하기" /><meta name="twitter:site" content="@twitter_username" /><meta name="twitter:creator" content="@Gukwon Koo" /><meta name="google-site-verification" content="google_meta_tag_verification" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Gukwon Koo"},"dateModified":"2025-08-31T18:12:54+09:00","datePublished":"2025-08-30T14:52:00+09:00","description":"T-SNE는 고차원의 임베딩을 저차원으로 변환하고 시각화화여 임베딩을 품질들 대략적으로 파악하는 것을 돕는 대표적인 차원 축소 알고리즘입니다. 실제로 아직도 현업에서도 많이 사용되는 알고리즘이고, 여러 논문에서도 임베딩의 질을 시각적으로 보여주기 위한 방법으로 많이 활용되고 있습니다.","headline":"T-SNE 이해하기","mainEntityOfPage":{"@type":"WebPage","@id":"https://gguguk.github.io/posts/tsne/"},"url":"https://gguguk.github.io/posts/tsne/"}</script><title>T-SNE 이해하기 | 생각과 고민.</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="생각과 고민."><meta name="application-name" content="생각과 고민."><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column align-items-end" lang="en"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="/assets/img/sample/bear.png" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">생각과 고민.</a></div><div class="site-subtitle font-italic">주니어 데이터 사이언티스트입니다.</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tags ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center"> <a href="https://github.com/gguguk" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github-alt"></i> </a> <a href="https://www.linkedin.com/in/%EA%B5%AD%EC%9B%90-%EA%B5%AC-32a9691a1/" aria-label="linkedin" target="_blank" rel="noopener"> <i class="fab fa-linkedin"></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>T-SNE 이해하기</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>T-SNE 이해하기</h1><div class="post-meta text-muted d-flex flex-column"><div> <span class="semi-bold"> Gukwon Koo </span> on <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="Sat, Aug 30, 2025, 2:52 PM +0900" >Aug 30<i class="unloaded">2025-08-30T14:52:00+09:00</i> </span></div><div> <span> Updated <span class="timeago lastmod" data-toggle="tooltip" data-placement="bottom" title="Sun, Aug 31, 2025, 6:12 PM +0900" >Aug 31<i class="unloaded">2025-08-31T18:12:54+09:00</i> </span> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="3417 words">18 min read</span></div></div><div class="post-content"><p>T-SNE는 고차원의 임베딩을 저차원으로 변환하고 시각화화여 임베딩을 품질들 대략적으로 파악하는 것을 돕는 대표적인 차원 축소 알고리즘입니다. 실제로 아직도 현업에서도 많이 사용되는 알고리즘이고, 여러 논문에서도 임베딩의 질을 시각적으로 보여주기 위한 방법으로 많이 활용되고 있습니다.</p><p>그러나 어느 순간 T-SNE의 원리를 완벽하게 이해하지 못하고 사용하고 있다는 생각이 들었습니다. 🫠 앞으로 현업에서 임베딩을 다룰 일이 많아질 것 같은 느낌이 들어서 이번 기회에 정확한 동작 원리를 공부해보았습니다. 공부해보니 ML 기초 이론을 복습하기에도 좋은 알고리즘이라는 생각이 들더라구요. 앞으로 설명 드릴 내용은 다음의 배경 지식을 요구합니다.</p><ul><li>Gaussian distribution<li>Student-T distribution<li>KL-Divergence<li>Gradient decent</ul><p><br /></p><h1 id="summary">Summary</h1><hr /><ul><li>T-SNE는 고차원 데이터를 저차원으로 내릴 때, 주변 이웃 정보를 최대한 보존하는 알고리즘입니다.<li>고차원에서 가까운 데이터는 저차원에서도 가깝게 만드는 것이 목표입니다. 이를 위해 기초 통계, 최적화 이론을 활용합니다.</ul><p><br /></p><h1 id="t-sne--gaussian--student-t--kl-divergence--gradient-decent">T-SNE = Gaussian + Student T + KL-Divergence + Gradient Decent</h1><hr /><p>T-SNE의 핵심 작동 원리는 4개의 키워드로 표현할 수 있습니다.</p><ul><li><strong>Gaussian distribution</strong>: 고차원 공간(원래 벡터들이 존재하는 공간)을 가우시안 분포로 표현합니다. 이때 원래 가까운 벡터들은 높은 확률값을 갖도록, 그 반대는 반대가 되도록 합니다.<li><strong>Student T-distribution</strong>: 저차원 공간(시각화를 위한 공간, 일반적을 2~3차원)을 T 분포로 표현합니다. 이때 고차원에서 원래 가까운 벡터들은 높은 확률을 가지도록, 그 반대는 반대가 되도록 표현하는 T 분포를 찾는 것이 목표입니다.<li><strong>KL-Divergence</strong>: 가우시안 분포와 T 분포가 차이가 나는 정도를 계산합니다.<li><strong>Gradient Decent</strong>: KL-Divergence의 값에 대한 저차원 벡터의 gradient를 계산하고, KL-Divergence가 작아지는 방향으로 저차원 벡터를 점진적으로 이동시킵니다.</ul><p><br /></p><h1 id="예시로-학습-방식-직관적으로-이해해-보기">예시로 학습 방식 직관적으로 이해해 보기</h1><hr /><p>T-SNE의 학습 방식을 예시를 들어봅시다. 직관적으로 이해하기 쉬울 거예요. T-SNE는 먼저 고차원에서 각 데이터 포인트(예: 학생 A, B)가 서로 <strong>얼마나 가까운지를 계산</strong>합니다.</p><ul><li>가까우면: 이 둘은 서로 비슷하네<li>멀면: 이 둘은 서로 다르네</ul><p><br /></p><p>같은 말을 <strong>확률</strong>로 표현하면 이렇게도 표현할 수 있습니다.</p><ul><li>가까우면: 이 둘이 서로 친구일 확률은 0.8<li>멀면: 이 둘이 서로 친구일 확률은 0.1</ul><p><br /></p><p>이제 데이터 포인트를 저차원 공간(예: 2차원)에 배치하는데, 이 때 <strong>위에서 계산한 친구일 확률을 최대한 유지하는 것</strong>이 중요합니다.</p><ul><li>고차원에서 <strong>가까웠다면</strong>: 저차원 공간에서도 서로 친구일 확률을 <strong>크게</strong><li>고차원에서 <strong>멀었다면</strong>: 저차원 공간에서도 서로 친구일 확률을 <strong>작게</strong></ul><p><br /></p><p>고차원 공간의 확률과 저차원 공간의 확률 구조를 비슷하게 유지하기 위해서 <strong>벌점을 부과</strong>합니다.</p><ul><li>고차원에서 가까웠지만(확률 큼) 저차원에서 멀면(확률 작음): 벌점이 커짐<li>고차원에서 가까웠고(확률 큼) 저차원에서도 가까우면(확률 큼): 벌점이 작아짐</ul><p><br /></p><p>마지막으로 이 <strong>벌점을 줄이기 위한 방향</strong>으로 <strong>저차원 공간의 데이터 포인터를 점진적으로 이동</strong>시킵니다.</p><ul><li>벌점이 큰 벡터: 벌점이 작아지는 방향으로 크게 이동 시킴<li>벌점이 작은 벡터: 벌점이 작아지는 방향으로 작게 이동 시킴</ul><p><br /></p><h1 id="수식과-함께-더-자세히-이해해-보기">수식과 함께 더 자세히 이해해 보기</h1><hr /><p>T-SNE의 학습 방식을 예시를 통해 직관적으로 이해해 보았습니다. 이제 수식을 곁들여서 좀 더 자세히 이해해 보죠!</p><h2 id="고차원-확률-분포-정의---가우시안-분포">고차원 확률 분포 정의 - 가우시안 분포</h2><p><img data-proofer-ignore data-src="/assets/img/post_img/normal_distribution_pdf.png" alt="" width="500" /><em>가우시안 분포</em></p><p>T-SNE는 고차원 공간에 데이터 포인트 \(x_i, \cdots, x_N\)가 있을 때(\(i \neq j\)) \(x_i\)와 \(x_j\)의 유사도(거리)에 비례하는 확률 분포 \(p_{ij}\)를 계산하는 것이 첫번째 목표입니다. 이를 위해 \(x_i\)가 \(x_j\)를 이웃으로 선택할 확률 \(p_{j \mid i}\)를 조건부 확률로 나타냅니다. 이 때, \(p_{i \mid i} = 0\)이고 \(\sum_{j}P_{j \mid i}=1, \forall i\)입니다.</p>\[p_{j \mid i} = \frac{\exp\!\left(-\| \mathbf{x}_i - \mathbf{x}_j \|^2 / 2\sigma_i^2 \right)} {\sum_{k \neq i} \exp\!\left(-\| \mathbf{x}_i - \mathbf{x}_k \|^2 / 2\sigma_i^2 \right)}\]<p>이 수식은 \(\mathbf{x}_i\)를 <strong>평균</strong>으로 하는 <strong>가우시안 분포</strong>의 수식과 사실상 같다는 것을 알 수 있습니다.</p>\[N(x; \mu, \sigma^2) = \cfrac{1}{\sqrt{2\pi\sigma^2}} \exp(- (x-\mu)^2/2\sigma^2)\]<p><br /></p><p>즉, 고차원 공간에서의 벡터의 거리를 확률로 변환하는데 이때 가우시안 분포를 활용한다는 것을 알 수 있습니다. 또한 \(p_{j \mid i}\)의 분모의 수식에 의해 식이 노말라이즈 되면서 \(\sum_{j}P_{j \mid i}=1, \forall i\)가 만족될 수 있음을 알 수 있습니다.</p><p><br /></p><p>마지막으로 조건부 확률을 대칭 확률로 변환하여 이는 \(x_i\)와 \(x_j\)가 이웃일 최종 확률을 구합니다. 이렇게 하는 이유는 \(p_{j \mid i}\)와 \(p_{i \mid j}\)의 분포가 다르기 때문입니다(분모가 다릅니다).</p>\[p_{ij} = \cfrac{p_{j \mid i} + p_{i \mid j}}{2N} \text{, where N=number of dimension}\]<p><br /></p><p>아직 \(p_{j \mid i}\)을 계산하기 위해 필요한 \(\sigma_i^2\)를 설명하지 않았습니다. 이 값은 \(x_i\)를 중심으로 한 가우시안 분포의 분산입니다. 하지만 이 값은 아직 정확히 계산할 수는 없습니다. 이렇게 생각할 수도 있습니다.</p><blockquote><p>“그냥 기초적인 분산 공식에 따라 계산할 수 있는 거 아니야?”</p></blockquote><p><br /></p><p>사실은 맞는 말입니다. 그러나 T-SNE 알고리즘은 이렇게 하진 않고, <strong>실질적으로 몇 개의 이웃 데이터 포인트만 중요하게 생각할지에 따라서 분산을 결정</strong>하는 전략을 취하고 있습니다. 그리고 이 분산을 정하기 위해서 perplexity라는 하이퍼파라미터를 도입합니다. 직관적으로 생각 해보면, 더 많은 데이터 포인트를 실질적인 이웃이라고 고려할수록 분산은 커지고, 그 반대는 반대가 될 겁니다. 여기서는 이 하이퍼파라미터가 왜 필요한지만 직관적으로 이해하고 넘어가고, perplexity는 아래에서 더 자세히 설명하겠습니다.</p><p><br /></p><h2 id="저차원-확률-분포-정의---t-분포">저차원 확률 분포 정의 - T 분포</h2><p><img data-proofer-ignore data-src="/assets/img/post_img/student_t_pdf.png" alt="" width="500" /><em>자유도에 따른 Student-t 분포</em></p><p>저차원에서는 T 분포에 기반하여 두 벡터간의 거리를 확률의 개념으로 표현하는 것만 제외하면 위 접근법과 거의 유사합니다. 저차원 공간에 \(y_1, \cdots, y_N (y_i \in \mathbb{R}^d)\) 데이터 포인트가 존재할 때, 아래 수식으로 두 데이터 포인트 \(y_i\)와 \(y_j\) 사이의 거리(유사도)를 확률 \(q_{ij}\)로 변환합니다.</p>\[q_{ij} = \frac{(1 + \| \mathbf{y}_i - \mathbf{y}_j \|^2)^{-1}} {\sum_k \sum_{l \neq k} (1 + \| \mathbf{y}_k - \mathbf{y}_l \|^2)^{-1}}\]<p><br /></p><p>이 수식은 자유도가 1인 student t-분포와 사실상 같다는 것을 알 수 있습니다. 먼저 student t-분포의 PDF는 다음과 같습니다.</p>\[f(t; \nu) = \frac{\Gamma\!\left(\tfrac{\nu+1}{2}\right)}{\sqrt{\pi \nu}\,\Gamma\!\left(\tfrac{\nu}{2}\right)}\left(1 + \frac{t^2}{\nu}\right)^{-\tfrac{\nu+1}{2}}\]<p><br /></p><p>이때 \(\nu = 1\)이라고 한다면, 다음과 같이 표현할 수 있습니다.</p>\[f(t) = c\left(1 + t^2\right)^{-1}\]<p><br /></p><h3 id="저차원-데이터-포인트-업데이트---kl-divergence--gradient-descent">저차원 데이터 포인트 업데이트 - KL-Divergence &amp; gradient descent</h3><p>지금까지 논의를 통해서 원래 고차원 공간에서의 데이터 포인트 간의 거리를 확률로 표현하고, 비슷한 방법으로 저차원 공간에서의 데이터 포인트 간의 거리를 확률로 표현하는 방법을 살펴 보았습니다. 그렇다면, 저차원 공간에서 결정된 데이터 포인트가 잘 사상된 것인지 어떻게 검증할 수 있을까요? 그리고 데이터 포인트를 더 적절한 위치로 옮기려면 어떻게 해야할까요? 이때 딥러닝의 학습의 기반이 되는 유명한 KL-Divergence와 gradient descent가 활용됩니다.</p><p><br /></p><p>먼저, KL-Divergence는 아래의 수식이고, 이 수식은 확률 분포 \(p_{ij}\)와 \(q_{ij}\)의 차이를 측정하게 됩니다.</p>\[KL(P \parallel Q) = \sum_{i \neq j} p_{ij} \log \frac{p_{ij}}{q_{ij}}\]<p><br /></p><p>마지막으로 계산된 KL-Divergence를 \(y_{i}\)에 대해 미분하여 gradient를 얻고, 이를 활용하여 \(y_i\)의 값을 변경하는 gradienct descent를 적용하게 됩니다. 결과적으로 따라서 T-SNE는 고차원 공간에서의 분포를 잘 반영할 수 있는 저차원 공간을 찾는 차원 축소 알고리즘이라고 볼 수 있습니다.</p>\[y_i \leftarrow y_i - \eta \cfrac{\partial KL(P \parallel Q)}{\partial y_i}{}\]<p><br /></p><h2 id="perplexity">Perplexity</h2><p>perplexity란 \(x_i\)가 <strong>실질적으로 혹은 중요하게 고려하는</strong> 이웃 데이터 포인트, <strong>즉 유효 이웃 수를 의미</strong>합니다. 이 값이 중요한 이유는 이에 따라 \(\sigma_i^2\)가 결정되기 때문입니다.</p><ul><li>perplexity가 클수록 분산 \(\sigma_i^2\)도 커집니다.<li>perplexity가 작을수록 분산 \(\sigma_i^2\)도 작아집니다.</ul><p>위에서 이론적으로는 분산 공식에 따라 \(\sigma_i^2\)를 구할 수는 있지만, T-SNE는 그렇게 하지 않는다고 언급 했습니다. T-SNE는 굳이 perplexity라는 하이퍼파라미터를 통해 적절한 분산 \(\sigma_i^2\)을 구합니다. 도대체 이유가 뭘까요? 그 이유는 데이터의 지역 밀도(local density)를 고려해서, <strong>모든 데이터 포인트가 유사한 유효 이웃 수를 갖도록 균형을 맞추기 위함</strong>입니다.</p><p>아래 그래프는 perplexity=5일 때, <strong>밀집 영역에 있는 빨강 데이터 포인트</strong>와 <strong>희소 영역에 있는 파랑 데이터 포인트</strong>에 대해서 분산 공식에 따라 <strong>전역 분산을 구했을 때의 가우시안 분포(점선)</strong>와 perplexity에 따라 <strong>국소 분산을 구했을 때의 가우시안 분포(실선)</strong>을 시각화 했습니다.</p><p><img data-proofer-ignore data-src="/assets/img/post_img/tsne_perplexity_visualize.png" alt="" width="750" /><em>perplexity=5일 때, 지역 밀집도에 따른 가우시안 분포</em></p><ul><li>국소 분산(실선): 데이터의 밀집도를 반영하여 <strong>본인 주위의 일부만을 유효 이웃으로 고려</strong>하고 있습니다.<li>전역 분산(점선):데이터의 밀집도를 반영하지 못하고, <strong>클러스터 내의 모든 데이터 포인트를 유효 이웃으로 고려</strong>하고 있습니다.</ul><p><strong>정리하면, T-SNE는 데이터의 지역 구조를 보존하기 위해 일반적인 분산 공식에 의한 전역 분산을 사용하지 않고, perplexity라는 값에 기반한 국소 분산을 활용한다고 할 수 있습니다.</strong> 일반적으로 perplexity는 5~50 사이의 값으로 설정한다고 합니다.</p><p>perplexity는 shannon entropy에 기반한 다음의 공식을 따릅니다.</p>\[\text{Perplexity}(P_i) = 2^{-\sum p_{j \mid i} \log_2{p_{j \mid i}}}\] \[\text{where} -\sum p_{j \mid i} \log_2{p_{j \mid i}} \text{ is Shannon entropy}\]<p><br /></p><p>만약 \(\text{Perplexity}(P_i)=5\)라고 설정 했으면, \(2^{-\sum p_{j \mid i} \log_2{p_{j \mid i}}}=5\)를 만족하는 분산 \(\sigma_i^2\)을 찾으면 됩니다.</p><p><br /></p><h1 id="t-sne의-단점과-변형-알고리즘들">T-SNE의 단점과 변형 알고리즘들</h1><hr /><p>T-SNE의 최대 단점은 데이터 포인트가 \(N\)개 일때, 한 이터레이션에서 \(N^2\)개의 쌍을 비교하는 연산이 필요하다는 것입니다. <strong>즉, 시간 복잡도가 \(O(N^2)\)</strong>입니다. 이를 극복하기 위해 다양한 변형 알고리즘들이 있으니 참고해보세요.</p><ul><li>Barnes-Hut t-SNE: \(O(N \log N)\)<li>Flt-SNE: \(O(N)\)</ul><p><br /></p><h1 id="laurens-van-der-maatent-sne-저자의-faq">Laurens van der Maaten(T-SNE 저자)의 FAQ</h1><hr /><h3 id="how-should-i-set-the-perplexity-in-t-sne">How should I set the perplexity in t-SNE?</h3><ul><li>The performance of t-SNE is fairly robust under different settings of the perplexity.<li>The most appropriate value depends on the density of your data. Loosely speaking, one could say that a larger / denser dataset requires a larger perplexity.<li>Typical values for the perplexity range between 5 and 50.</ul><h3 id="what-is-perplexity-anyway">What is perplexity anyway?</h3><ul><li>In t-SNE, the perplexity may be viewed as a knob that sets the number of effective nearest neighbors.<li>It is comparable with the number of nearest neighbors k that is employed in many manifold learners.</ul><h3 id="every-time-i-run-t-sne-i-get-a-slightly-different-result">Every time I run t-SNE, I get a (slightly) different result?</h3><ul><li>In contrast to, e.g., PCA, t-SNE has a non-convex objective function.<li>The objective function is minimized using a gradient descent optimization that is initiated randomly. As a result, it is possible that different runs give you different solutions.<li>Notice that it is perfectly fine to run t-SNE a number of times (with the same data and parameters), and to select the visualization with the lowest value of the objective function as your final visualization.</ul><p><br /></p><h1 id="참고자료">참고자료</h1><ul><li><a href="https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding#External_links">t-distributed stochastic neighbor embedding</a><li><a href="https://en.wikipedia.org/wiki/Student%27s_t-distribution">Student’s t-distribution</a><li>https://lvdmaaten.github.io/tsne/</ul></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/ml/'>ML</a>, <a href='/categories/basic/'>Basic</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/tsne/" class="post-tag no-text-decoration" >tsne</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=T-SNE 이해하기 - 생각과 고민.&url=https://gguguk.github.io/posts/tsne/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=T-SNE 이해하기 - 생각과 고민.&u=https://gguguk.github.io/posts/tsne/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=T-SNE 이해하기 - 생각과 고민.&url=https://gguguk.github.io/posts/tsne/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" title-succeed="Link copied successfully!"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"> <span>Recent Update</span><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/tsne/">T-SNE 이해하기</a><li><a href="/posts/how_to_work_python/">CS50 - 파이썬이 소스 코드를 실행하는 과정과 원리</a><li><a href="/posts/OIDC/">IRSA의 원리를 파헤쳐보자 4 - OIDC</a><li><a href="/posts/OAuth/">IRSA의 원리를 파헤쳐보자 3 - OAuth2.0</a><li><a href="/posts/admission_webhook/">IRSA의 원리를 파헤쳐보자 1 - K8S Admission Webhook</a></ul></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"> <span class="pl-3 pt-2 mb-2">Contents</span><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/logit_sigmoid_softmax/"><div class="card-body"> <span class="timeago small" >Jun 7, 2020<i class="unloaded">2020-06-07T00:00:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Ligit, Sigmoid, Softmax의 관계</h3><div class="text-muted small"><p> 왜 NN의 출력층에 sigmoid, softmax 함수를 사용할까요? 이는 출력층의 값을 ‘확률’로서 표현하기 위한 필연적 결과입니다. 본 글에서는 logit, sigmoid, softmax의 관계에 대해서 정리해보았습니다. 1. Basic 앞으로 내용을 전개할 때 중요하게 사용되는 bayes theorem(베이즈 정리) 및 law of to...</p></div></div></a></div><div class="card"> <a href="/posts/python_package_init/"><div class="card-body"> <span class="timeago small" >Oct 31, 2021<i class="unloaded">2021-10-31T17:09:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>오픈 소스 패키지 분석을 위한 __init__.py 알아보기</h3><div class="text-muted small"><p> 오픈 소스 툴을 사내에 도입하기 위해 분석하던 중에 알게된 파이썬의 __init__.py 파일을 역할을 정리합니다. 먼저 왜 __init__.py의 역할과 기능을 정리하게 되었는지를 말씀드리고, 이를 이해하기 위한 배경지식을 말씀드리겠습니다. 그리고 __init__.py의 다양한 역할에 대해 정리합니다. 마지막으로 여러분들께 친숙한 pandas 패키지...</p></div></div></a></div><div class="card"> <a href="/posts/nlp_transfer_learning_history/"><div class="card-body"> <span class="timeago small" >Mar 5, 2020<i class="unloaded">2020-03-05T00:00:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>NLP Transfer Learning History</h3><div class="text-muted small"><p> 김성현 연구원님의 T아카데미 토크ON세미나 ‘딥러닝 기반의 자연어 언어 모델 BERT’ 라는 세미나를 듣고 알게된 정보와 구글링을 통해 알게된 정보를 종합하여 관련 내용을 정리해 보고자 합니다. 1   From word embedding To pretrained language models 1.1   Traditional context-fr...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/OIDC/" class="btn btn-outline-primary" prompt="Older"><p>IRSA의 원리를 파헤쳐보자 4 - OIDC</p></a> <span class="btn btn-outline-primary disabled" prompt="Newer"><p>-</p></span></div></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2025 <a href="https://github.com/gguguk">Gukwon Koo</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">Trending Tags</h4><a class="post-tag" href="/tags/python/">python</a> <a class="post-tag" href="/tags/retrospective/">retrospective</a> <a class="post-tag" href="/tags/k8s/">k8s</a> <a class="post-tag" href="/tags/kubernetes/">kubernetes</a> <a class="post-tag" href="/tags/nlp/">nlp</a> <a class="post-tag" href="/tags/statistics/">statistics</a> <a class="post-tag" href="/tags/irsa/">irsa</a> <a class="post-tag" href="/tags/ml/">ml</a> <a class="post-tag" href="/tags/mlops/">mlops</a> <a class="post-tag" href="/tags/paper/">paper</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="https://gguguk.github.io{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/lozad/dist/lozad.min.js,npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.16.1,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id=G-8EWVG7CHCY"></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'G-8EWVG7CHCY'); }); </script>

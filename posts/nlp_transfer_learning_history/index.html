<!DOCTYPE html><html lang="en" mode="light" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta http-equiv="Cache-Control" content="no-cache, no-store, must-revalidate"><meta http-equiv="Expires" content="0"><meta http-equiv="Pragma" content="no-cache"><meta name="day-prompt" content="days ago"><meta name="hour-prompt" content="hours ago"><meta name="minute-prompt" content="minutes ago"><meta name="justnow-prompt" content="just now"><meta name="generator" content="Jekyll v4.4.1" /><meta property="og:title" content="NLP Transfer Learning History" /><meta name="author" content="Gukwon Koo" /><meta property="og:locale" content="en" /><meta name="description" content="김성현 연구원님의 T아카데미 토크ON세미나 ‘딥러닝 기반의 자연어 언어 모델 BERT’ 라는 세미나를 듣고 알게된 정보와 구글링을 통해 알게된 정보를 종합하여 관련 내용을 정리해 보고자 합니다." /><meta property="og:description" content="김성현 연구원님의 T아카데미 토크ON세미나 ‘딥러닝 기반의 자연어 언어 모델 BERT’ 라는 세미나를 듣고 알게된 정보와 구글링을 통해 알게된 정보를 종합하여 관련 내용을 정리해 보고자 합니다." /><link rel="canonical" href="https://gguguk.github.io/posts/nlp_transfer_learning_history/" /><meta property="og:url" content="https://gguguk.github.io/posts/nlp_transfer_learning_history/" /><meta property="og:site_name" content="생각과 고민." /><meta property="og:type" content="article" /><meta property="article:published_time" content="2020-03-05T00:00:00+09:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="NLP Transfer Learning History" /><meta name="twitter:site" content="@twitter_username" /><meta name="twitter:creator" content="@Gukwon Koo" /><meta name="google-site-verification" content="google_meta_tag_verification" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Gukwon Koo"},"dateModified":"2020-03-05T00:00:00+09:00","datePublished":"2020-03-05T00:00:00+09:00","description":"김성현 연구원님의 T아카데미 토크ON세미나 ‘딥러닝 기반의 자연어 언어 모델 BERT’ 라는 세미나를 듣고 알게된 정보와 구글링을 통해 알게된 정보를 종합하여 관련 내용을 정리해 보고자 합니다.","headline":"NLP Transfer Learning History","mainEntityOfPage":{"@type":"WebPage","@id":"https://gguguk.github.io/posts/nlp_transfer_learning_history/"},"url":"https://gguguk.github.io/posts/nlp_transfer_learning_history/"}</script><title>NLP Transfer Learning History | 생각과 고민.</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="생각과 고민."><meta name="application-name" content="생각과 고민."><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column align-items-end" lang="en"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="/assets/img/sample/bear.png" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">생각과 고민.</a></div><div class="site-subtitle font-italic">주니어 데이터 사이언티스트입니다.</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tags ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center"> <a href="https://github.com/gguguk" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github-alt"></i> </a> <a href="https://www.linkedin.com/in/%EA%B5%AD%EC%9B%90-%EA%B5%AC-32a9691a1/" aria-label="linkedin" target="_blank" rel="noopener"> <i class="fab fa-linkedin"></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>NLP Transfer Learning History</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>NLP Transfer Learning History</h1><div class="post-meta text-muted d-flex flex-column"><div> <span class="semi-bold"> Gukwon Koo </span> on <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="Thu, Mar 5, 2020, 12:00 AM +0900" >Mar 5, 2020<i class="unloaded">2020-03-05T00:00:00+09:00</i> </span></div><div> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="2439 words">13 min read</span></div></div><div class="post-content"><blockquote><p>김성현 연구원님의 T아카데미 토크ON세미나 ‘딥러닝 기반의 자연어 언어 모델 BERT’ 라는 세미나를 듣고 알게된 정보와 구글링을 통해 알게된 정보를 종합하여 관련 내용을 정리해 보고자 합니다.</p></blockquote><h2 id="1--from-word-embedding-to-pretrained-language-models">1   From word embedding To pretrained language models</h2><h3 id="11--traditional-context-free-representation">1.1   Traditional context-free representation</h3><h4 id="111--one-hot-encoding">1.1.1   One-hot encoding</h4><ul><li>특징: 단어를 수치화하여 표현하기 위해서 단어 사전 크기의 차원을 가지는 벡터를 만들고 단어의 인덱스에 해당하는 원소만 1, 나머지는 0으로 하여 표현<li>단점: 단어의 의미(meaning)를 인코딩 할 수 없음, 단어끼리의 유사성을 계산할 수 없음, 매우 sparse한 벡터 표현으로 연산 자원의 낭비를 초래함</ul><h4 id="112--tf-idf-representation---통계-기반-기법">1.1.2   TF-IDF representation - 통계 기반 기법</h4><ul><li>특징: 통계적 접근 방법으로서 TF(term frequency)와 IDF(inverse document frequency) 값을 이용하여 단어의 document 내에서의 중요도를 표현할 수 있는(수치화 할 수 있는) 방법<li>단점: 단어의 의미(meaning)을 인코딩 할 수 없음, one-hot encoding 보다는 덜 하지만 여전히 sparse한 벡터 표현으로 연산 자원의 낭비를 초래함</ul><p><br /></p><h4 id="12--distributed-similarity-based-representation-word-embeddings">1.2   Distributed similarity based representation: Word embeddings</h4><h4 id="121--word2vec---추론-기반-기법">1.2.1   word2vec - 추론 기반 기법</h4><ul><li>특징: 어떤 단어의 의미는 주변 단어들에 의해 결정된다는 분포가설(distributed hypothesis)에 기초하여 은닉층이 1개인 간단한 인공신경망을 통해 context 단어를 통해 target 단어를 예측 하거나(CBOW), target 단어를 이용하여 context 단어를 예측(skip-gram) 방식을 통해 확률 분포를 모델링함. 그 결과로 얻어진 은닉층 가중치 벡터를 단어의 representation으로 활용함<li>장점: 단어의 의미(meaning)를 인코딩 할 수 있고, 단어 사이의 유사성도 추론하는 작업을 할 수 있음<li>단점: OOV(out of vocabulary) 단어들을 처리할 수 없음. 동음이의어처럼 ‘문맥(context)’에 뜻이 달라질 수 있는 상황을 고려하지 못함. subword information이 무시됨(‘서울’, ‘서울시’ 두 단어를 서로 독립된 두 단어로 취급함). 단어의 DF가 낮을 경우(corpus에서 등장 빈도 자체가 적을 경우) 유사한 단어를 찾지 못함</ul><p><br /></p><h4 id="122--gloveglobal-vector---추론-기반--통계-기반-기법">1.2.2   Glove(Global vector) - 추론 기반 + 통계 기반 기법</h4><ul><li>특징: word2vec과 같은 방법은 window size내의 맥락들만 고려 할 수 있고 corpus의 전역적인 특성을 고려하지 못한다는 단점이 존재. 따라서 학습시 corpus 전체의 통계 정보(동시 발생 행렬, co-occurence matrix)를 손실함수에 도입하여 이와 같은 단점을 해소하고자 함<li>장점: ?<li>단점: 여전히 OOV 단어를 처리할 수 없음. 단어의 문맥에 따른 의미 변화를 반영할 수 없음.</ul><p><br /></p><h4 id="123--fasttext">1.2.3   Fasttext</h4><ul><li>특징: facebook research에서 공개함. word2vec과 유사하지만, 그 과정에서 단어를 n-gram으로 나누어서 학습을 진행한다는 점에서 차이점이 존재함.<li>장점: OOV에 대응할 수 있음(테스트 단어가 학습 시의 사전에 존재하면 해당 단어의 벡터를 그대로 리턴하지만, OOV일 경우 입력 단어의 n-gram 벡터들의 합을 리턴함). OOV에 대응할 수 있다는 것은 학습시 사용되지 않았지만 오탈자가 없는 ‘정상적인 단어’에 대해서도 대응을 할 수 있다는 것 외에도 ‘오탈자’에 대해서도 대응을 할 수 있다는 것을 의미함. subword information을 고려할 수 있기 때문에 DF가 낮은 단어도 word2vec에 비해 유사한 단어를 잘 찾을 수 있음<li>단점: 여전히 단어의 문맥에 따른 의미의 변화를 고려할 수 없음</ul><p><br /></p><h2 id="2--contextualized-representations">2   contextualized representations</h2><p>context-free representation 방법론은 공통적으로 한 단어의 의미가 항상 고정(stable)하다고 보는 큰 한계점이 있었다. 다시말해 동음이의어인 단어를 여러가지 의미로 보는 것이 아니라 단어의 ‘형태’가 같으면 같은 의미로 파악하는 것이다. 이것을 개선하기 위해서 언어모델(language model)에 기반한 contextualize representation 방법론이 등장하게 되었다.</p><p>NLP task의 데이터셋의 크기는 높은 성능을 얻기 위한 재료의 관점에서 볼 때, 그 사이즈가 부족한 것이 현실인데 이 때문에 NLP task에 사용할 neural network를 깊게 디자인 하기 힘들다(작은 데이터셋에 깊은 신경망을 쓰면 overfit이 발생하고 일반화 성능을 얻기 힘듦). 반면에 vision task의 경우 대규모의 학습 데이터셋인 ImageNet이 공개 되어 있다. 따라서 자신의 실제 task에 적용시키기 전에 pre-training을 통해 일반적인 이미지 feature를 추출는 방법을 모델로 하여금 학습하도록 하고, 실제 task에서는 소규모의 데이터셋으로 fine-tuning을 통해 높은 성능을 끌어낼 수 있었다.</p><p>NLP task에서도 소규모 데이터셋을 이용하면서도 높은 성능을 이끌어 내기 위해 pre-training 방법론이 제안되어져 왔다. 이는 언어 모델(language model)을 통해서 실현이 가능하다.</p><p>NLP task를 진행하는 전통적인 방법은 word embedding 기술을 활용하여 학습을 수행할 모델의 input값을 초기화 시킨 후 모델의 학습을 진행하는 것이었다면, 최근의 NLP task의 흐름은 pre-trained language model(문장 내에서 단어들의 관계성을 high-level 수준으로 이미 알고 있는 모델)을 활용하여 마지막 레이어만 특정 task에 맞게 수정하여 사용하는 방식으로 바뀌었다.</p><h3 id="21--elmoembeddings-from-language-models">2.1   ELMo(Embeddings from Language Models)</h3><p>문장에 등장하는 단어는 그 문맥에 따라 다른 의미를 지니고 있는 경우가 많다. 예를 들어 ‘사과’는 ‘사과가 맛있다.’라는 문장에서는 ‘과일’을 의미하며, ‘그녀에서 나의 잘못을 사과하였다.’라는 문장에서는 ‘자기의 잘못을 인정하고 용서를 빎’이라는 의미이다. 이처럼 자연어는 형태는 같지만 다양한 의미를 지니고 있다. 다시 말해서 해당 단어가 어떤 <strong>문맥</strong> 에서 사용되었는지에 따라 그 의미가 다르다는 것이다. 그러나 앞서 언급한 것처럼 그간 word prepresentaion 방법은 어떤 문맥에서 단어가 상관되었는지에 관계없이 <strong>고정된 하나의 벡터</strong>만을 가질 수 있었다.</p><p>ELMo는 그간의 word representation이 문맥을 반영하지 못한다는 한계점을 지적하며, 문맥에 따라 representation을 다르게 하여 context를 반영한 representation 방법을 제안한다. ELMo는 NLP의 전이학습의 시작점을 제공한 모델로서, bi-directional LSTM 모델을 통해 다음에 올 토큰을 예측하는 언어 모델(language model)이다.</p><p><br /></p><h3 id="22--ulmfituniversal-language-model-fine-tuning">2.2   ULMFiT(Universal Language Model Fine-tuning)</h3><p><br /></p><h3 id="23--transformer">2.3   Transformer</h3><p>NMT(neural machine translation) 문제를 해결하기 위해 고안된 모델로서 기존의 NMT에서 주로 사용되던 LSTM이 가지는 병렬연산 불가 문제를 해결하였다. seq2seq 모델과 마찬가지로 encoder와 decoder 구조로 이루어져 있지만, RNN 계열의 cell을 사용하지 않고 이를 transformer로 대체하였다. 특히 self-attention 구조를 제안한 것이 특징이다. 기존의 seq2seq with attention 모델의 경우에는 source sentence를 target sentence로 변환하는 과정에서 target word와 source word 관계에 대해서만 설명할 수 있었고(디코더의 출력이 인코더의 어느 부분에 주목한 결과인지 알 수 있음). 그러나 source word 간의 관계에 대해서를 설명할 수 없는 단점이 있었다. 예를 들어 “나는 배가 고파서 사과를 먹었는데, 그것은 상해 있었다.”라는 문장에 대해서 ‘사과’와 ‘그것’의 관계를 주목할 수 없었다는 것이다. transformer는 self-attention 구조를 통해서 이를 해결하였다. 이후에 등장하는 GPT, BERT는 NMT 문제에 적용된 transfomer를 어떻게 downstream task에 적용 할 수 있을지를 고민하여 탄생한 모델이다.</p><p><br /></p><h3 id="24--gptgenerative-pre-training-transformer">2.4   GPT(Generative Pre-Training Transformer)</h3><p>transfomer의 decoder 부분만 활용하였다.</p><p><br /></p><h3 id="25--bertbidirectional-encoder-representation-from-transformer">2.5   BERT(Bidirectional Encoder Representation from Transformer)</h3><p>지금까지 transformer 기반의 모델은 ELMo가 제안하였던 bi-directional 방법을 고려하지 못하였다. BERT는 transformer의 encoder만을 활용한 모델인데, 인풋 토큰의 일부를 가리는 mask 토큰을 삽입하여 모든 문맥을 고려한(mask 토큰의 앞과 뒤) mask language model이다. 이를 통해 transformer와 ELMo의 장점을 모두 취할 수 있었다.</p><p><br /></p><h2 id="참고자료">참고자료</h2><ul><li>김성현, T아카데미 토크ON 세미나 - 딥러닝 기반의 자연어 언어 모델 BERT<li><a href="https://towardsdatascience.com/from-word-embeddings-to-pretrained-language-models-a-new-age-in-nlp-part-1-7ed0c7f3dfc5">From Word Embeddings to Pretrained Language Models — A New Age in NLP</a><li>[The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)](</ul></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/ml/'>ML</a>, <a href='/categories/nlp/'>NLP</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/nlp/" class="post-tag no-text-decoration" >nlp</a> <a href="/tags/transfer-learning/" class="post-tag no-text-decoration" >transfer learning</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=NLP Transfer Learning History - 생각과 고민.&url=https://gguguk.github.io/posts/nlp_transfer_learning_history/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=NLP Transfer Learning History - 생각과 고민.&u=https://gguguk.github.io/posts/nlp_transfer_learning_history/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=NLP Transfer Learning History - 생각과 고민.&url=https://gguguk.github.io/posts/nlp_transfer_learning_history/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" title-succeed="Link copied successfully!"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"> <span>Recent Update</span><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/tsne/">T-SNE 이해하기</a><li><a href="/posts/how_to_work_python/">CS50 - 파이썬이 소스 코드를 실행하는 과정과 원리</a><li><a href="/posts/OIDC/">IRSA의 원리를 파헤쳐보자 4 - OIDC</a><li><a href="/posts/OAuth/">IRSA의 원리를 파헤쳐보자 3 - OAuth2.0</a><li><a href="/posts/admission_webhook/">IRSA의 원리를 파헤쳐보자 1 - K8S Admission Webhook</a></ul></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"> <span class="pl-3 pt-2 mb-2">Contents</span><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/byte_pair-encoding/"><div class="card-body"> <span class="timeago small" >Jun 5, 2020<i class="unloaded">2020-06-05T00:00:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Byte Pair Encoding</h3><div class="text-muted small"><p> 최근 NLP에서 tokenizer로 많이 사용되고 있는 BPE에 대해서 간단하게 정리해 보겠습니다. 전체코드는 이곳에서 확인해 보실 수 있습니다. 1   Backgroud: Subword Segmentation subword segmentation(단어 분리, 단어 분절)이란, 하나의 단어(혹은 토큰)는 여러 개의 subword의 조합으로 이...</p></div></div></a></div><div class="card"> <a href="/posts/fasttext/"><div class="card-body"> <span class="timeago small" >Aug 16, 2020<i class="unloaded">2020-08-16T00:00:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Enriching Word Vectors with Subword Information</h3><div class="text-muted small"><p> Fasttext 논문(Enriching Word Vectors with Subword Information)을 리뷰해 보았습니다. 슬라이드쉐어에 동일한 내용을 업로드 하였으며, 슬라이드쉐어에서 보시려면 이곳을 확인해 주세요. 1   Introduction 2   General Model 3   Subwor...</p></div></div></a></div><div class="card"> <a href="/posts/attention_is_all_you_need/"><div class="card-body"> <span class="timeago small" >Oct 16, 2020<i class="unloaded">2020-10-16T00:00:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Attention Is All You Need</h3><div class="text-muted small"><p> “Attention Is All You Need” 논문을 읽은 후 관련 자료를 정리한 내용을 바탕으로 논문 리뷰를 진행 해보겠습니다. 서론 및 문헌연구는 제외하겠습니다. 1     Model Architecture transformer에서 가장 중요한 block은 self-attention과 point-wise feed forward net...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <span class="btn btn-outline-primary disabled" prompt="Older"><p>-</p></span> <a href="/posts/MAB/" class="btn btn-outline-primary" prompt="Newer"><p>MAB와 Thomson Sampling</p></a></div></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2025 <a href="https://github.com/gguguk">Gukwon Koo</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">Trending Tags</h4><a class="post-tag" href="/tags/python/">python</a> <a class="post-tag" href="/tags/retrospective/">retrospective</a> <a class="post-tag" href="/tags/k8s/">k8s</a> <a class="post-tag" href="/tags/kubernetes/">kubernetes</a> <a class="post-tag" href="/tags/nlp/">nlp</a> <a class="post-tag" href="/tags/statistics/">statistics</a> <a class="post-tag" href="/tags/irsa/">irsa</a> <a class="post-tag" href="/tags/ml/">ml</a> <a class="post-tag" href="/tags/mlops/">mlops</a> <a class="post-tag" href="/tags/paper/">paper</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="https://gguguk.github.io{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/lozad/dist/lozad.min.js,npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.16.1,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id=G-8EWVG7CHCY"></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'G-8EWVG7CHCY'); }); </script>

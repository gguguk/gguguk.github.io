<!DOCTYPE html><html lang="en" mode="light" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta http-equiv="Cache-Control" content="no-cache, no-store, must-revalidate"><meta http-equiv="Expires" content="0"><meta http-equiv="Pragma" content="no-cache"><meta name="day-prompt" content="days ago"><meta name="hour-prompt" content="hours ago"><meta name="minute-prompt" content="minutes ago"><meta name="justnow-prompt" content="just now"><meta name="generator" content="Jekyll v4.4.1" /><meta property="og:title" content="tf.keras 커스텀 하기" /><meta name="author" content="Gukwon Koo" /><meta property="og:locale" content="en" /><meta name="description" content="데이터 사이언스, 추천 시스템, Data Science, Recommender" /><meta property="og:description" content="데이터 사이언스, 추천 시스템, Data Science, Recommender" /><link rel="canonical" href="https://gguguk.github.io/posts/custom_tensorflow_2/" /><meta property="og:url" content="https://gguguk.github.io/posts/custom_tensorflow_2/" /><meta property="og:site_name" content="생각과 고민." /><meta property="og:type" content="article" /><meta property="article:published_time" content="2021-02-17T21:26:00+09:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="tf.keras 커스텀 하기" /><meta name="twitter:site" content="@twitter_username" /><meta name="twitter:creator" content="@Gukwon Koo" /><meta name="google-site-verification" content="google_meta_tag_verification" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Gukwon Koo"},"dateModified":"2021-02-17T21:26:00+09:00","datePublished":"2021-02-17T21:26:00+09:00","description":"데이터 사이언스, 추천 시스템, Data Science, Recommender","headline":"tf.keras 커스텀 하기","mainEntityOfPage":{"@type":"WebPage","@id":"https://gguguk.github.io/posts/custom_tensorflow_2/"},"url":"https://gguguk.github.io/posts/custom_tensorflow_2/"}</script><title>tf.keras 커스텀 하기 | 생각과 고민.</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="생각과 고민."><meta name="application-name" content="생각과 고민."><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column align-items-end" lang="en"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="/assets/img/sample/bear.png" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">생각과 고민.</a></div><div class="site-subtitle font-italic">주니어 데이터 사이언티스트입니다.</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tags ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center"> <a href="https://github.com/gguguk" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github-alt"></i> </a> <a href="https://www.linkedin.com/in/%EA%B5%AD%EC%9B%90-%EA%B5%AC-32a9691a1/" aria-label="linkedin" target="_blank" rel="noopener"> <i class="fab fa-linkedin"></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>tf.keras 커스텀 하기</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>tf.keras 커스텀 하기</h1><div class="post-meta text-muted d-flex flex-column"><div> <span class="semi-bold"> Gukwon Koo </span> on <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="Wed, Feb 17, 2021, 9:26 PM +0900" >Feb 17, 2021<i class="unloaded">2021-02-17T21:26:00+09:00</i> </span></div><div> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="4480 words">24 min read</span></div></div><div class="post-content"><p><img data-proofer-ignore data-src="https://miro.medium.com/max/4928/1*-QTg-_71YF0SVshMEaKZ_g.png" alt="" /></p><p>최근에 python2 + tensorflow 1.x로 작성된 추천 시스템 레거시 코드를 유지 보수 및 개선하는 업무를 진행하고 있습니다. 기존 코드는 tensorflow 1.x 버전으로 짜여져 있어서 API의 통일성이 부족했고, 오픈 소스 코드에 기반하여 상황 마다 필요한 컴포넌트를 추가 하다보니 코드의 일관성도 많이 저해된 상태였습니다. 따라서 유지 보수의 용이성을 확보하기 위해 tensorflow를 2.0 버전으로 코드를 변환해야겠다는 결심을 하게 되었습니다. tensorflow 공식 문서에서는 tensorflow 2.0 버전의 특징이 다음의 네 가지로 요약되어 있습니다. 가독성이 향상되고, 디버깅도 편해질 것 같군요!</p><ul><li><a href="https://www.tensorflow.org/guide/effective_tf2#api_cleanup">API Cleanup</a><li><a href="https://www.tensorflow.org/guide/effective_tf2#eager_execution">Eager execution</a><li><a href="https://www.tensorflow.org/guide/effective_tf2#no_more_globals">No more globals</a><li><a href="https://www.tensorflow.org/guide/effective_tf2#functions_not_sessions">Functions, not session</a></ul><p><br /></p><p>특히 저는 깔끔하면서도 정형화된 형태로 tf 코드를 작성하고, 필요하다면 low-level로 layer를 커스터마이징할 수 있어야 했으므로 tensorflow 2.0 + keras layer 조합으로 코드를 작성하였습니다. 본 글에서는 이와 관련된 내용을 공부하면서 얻은 지식을 다음의 내용을 중심으로 정리해보겠습니다.</p><ul><li>custom layer class<li>custom loss class<li>custom model class<li>custom training loop</ul><p><br /></p><h1 id="layer-class">Layer Class</h1><hr /><p>keras layer를 만들 때, subclassing<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>을 활용하여 클래스 형태로 만들 수 있습니다. 이와 같은 방식에는 두가지 장점이 있다고 생각하는데요.</p><ul><li>메서드 이름이나 메서드가 받는 argument 등에 대한 정형화된 형태가 존재하고, 이에 맞춰 코드를 작성해야 하기 때문에 <strong>가독성이 향상</strong>됩니다.<li>한편으로 layer에서 이루어지는 computation 로직은 직접 low-level로 작성할 수 있기 때문에 <strong>커스터마이징</strong>이 가능합니다.</ul><p><br /></p><p>layer class는 weight와 computation의 결합으로 표현할 수 있는데요. 텐서플로우 <a href="https://www.tensorflow.org/guide/keras/custom_layers_and_models">공식 문서</a>에서는 layer class에 대해 다음과 같이 설명하고 있습니다. 한마디로 정리하면 weight와 computation 과정이 결합된 object라고 말할 수 있겠습니다.</p><blockquote><p>One of the central abstraction in Keras is the <code class="language-plaintext highlighter-rouge">Layer</code> class. A layer encapsulates both a state (the layer’s “weights”) and a transformation from inputs to outputs (a “call”, the layer’s forward pass).</p></blockquote><p><br /></p><p><a href="https://www.tensorflow.org/guide/keras/custom_layers_and_models#best_practice_deferring_weight_creation_until_the_shape_of_the_inputs_is_known">이 문서</a>에서는 custom layer class를 작성하는 <strong>best practice</strong>를 다음과 같이 제안하고 있습니다.</p><div class="language-python highlighter-rouge"><div class="code-header" text-data="python"><button data-original-title="Copied!"><i class="far fa-clone"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
</pre><td class="rouge-code"><pre><span class="k">class</span> <span class="nc">Linear</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Layer</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">units</span><span class="o">=</span><span class="mi">32</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">Linear</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">units</span> <span class="o">=</span> <span class="n">units</span>

    <span class="k">def</span> <span class="nf">build</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">w</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">add_weight</span><span class="p">(</span>
            <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">input_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">self</span><span class="p">.</span><span class="n">units</span><span class="p">),</span>
            <span class="n">initializer</span><span class="o">=</span><span class="sh">"</span><span class="s">random_normal</span><span class="sh">"</span><span class="p">,</span>
            <span class="n">trainable</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">b</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">add_weight</span><span class="p">(</span>
            <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">units</span><span class="p">,),</span> <span class="n">initializer</span><span class="o">=</span><span class="sh">"</span><span class="s">random_normal</span><span class="sh">"</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="bp">True</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">tf</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">b</span>
</pre></table></code></div></div><ul><li><p><strong>__init__()</strong></p><p>해당 layer에서 활용하는 hyperparameter 등을 선언합니다.</p><li><p><strong>build()</strong></p><p>해당 layer에서 활용하는 trainable/non-trainable weights와 관련된 로직을 작성합니다. 특히 build 메서드에 weights 초기화 로직을 구현하면 해당 레이어의 인풋의 차원을 정확하게 알지 못하더라도 lazy 하게 작동하게 할 수 있다는 장점이 있습니다. build 메서드에 선언된 weights는 call 메서드가 처음 호출될 때 생성됩니다.</p><li><p><strong>call()</strong></p><p>해당 layer의 computation과 관련된 로직을 작성합니다. 인풋을 받아 원하는 형태의 아웃풋을 리턴하도록 하면 되겠습니다.</p></ul><p><br /></p><p>그러나 딥러닝 코드를 짜다보면 같은 computation 로직을 반복 사용해야할 경우가 많죠. 예를 들어 위 예시처럼 Linear(Dense) layer는 정말 많이 사용하는데요. 내가 만든 custom layer class 혹은 keras layer class 여러 개를 묶어서 다시 레이어 클래스를 만드려면 어떻게 해야할까요? <a href="https://www.tensorflow.org/guide/keras/custom_layers_and_models#layers_are_recursively_composable">공식 문서</a>에서는 아래와 같은 방법을 제안합니다.</p><div class="language-python highlighter-rouge"><div class="code-header" text-data="python"><button data-original-title="Copied!"><i class="far fa-clone"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
</pre><td class="rouge-code"><pre><span class="k">class</span> <span class="nc">MLPBlock</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Layer</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">MLPBlock</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">linear_1</span> <span class="o">=</span> <span class="nc">Linear</span><span class="p">(</span><span class="mi">32</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">linear_2</span> <span class="o">=</span> <span class="nc">Linear</span><span class="p">(</span><span class="mi">32</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">linear_3</span> <span class="o">=</span> <span class="nc">Linear</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">linear_1</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">linear_2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">linear_3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></table></code></div></div><p>첫번째 예시와 거의 비슷한 형태인데요. 공통점은 computation 로직이 <code class="language-plaintext highlighter-rouge">call()</code> 메서드로 처리된다는 점입니다. 그러나 <code class="language-plaintext highlighter-rouge">build()</code> 메서드에 weight를 직접 선언하지 않고, layer class를 <code class="language-plaintext highlighter-rouge">__init__()</code> 메서드에 선언한다는 것이 차이점이네요. 이는 공식적으로 권장하는 방법입니다. 그 이유는 outer layer (e.g. MLPBlock)는 inner layer (e.g. Linear)의 weight를 자동으로 추적할수 있기 때문이라고 합니다. 이 부분은 공식 문서의 설명이 조금 부실한데요. outer layer class에서 <code class="language-plaintext highlighter-rouge">call()</code> 메서드가 처음으로 호출되면, computation 과정에 참여하는 inner layer들의 <code class="language-plaintext highlighter-rouge">call()</code> 메서드도 역시 처음으로 호출되게 됩니다. 이때 inner layer의 <code class="language-plaintext highlighter-rouge">build()</code> 메서드에 선언된 weight들이 lazy하게 생성됩니다. outer layer는 inner layer의 weight를 추적할 수 있도록 디자인 되어 있기 때문에 이와 같은 로직 작성이 가능한 것입니다.</p><p><br /></p><h1 id="loss-class">Loss Class</h1><hr /><p>custom loss function을 만드는 방법은 크게 세 가지로 나눌 수 있습니다.</p><ul><li>simple loss function<li>nested loss function<li>loss class</ul><p><br /></p><p>먼저 세가지 방식은 공통적으로 <strong>loss를 계산하는 메서드가 <code class="language-plaintext highlighter-rouge">y_true</code>, <code class="language-plaintext highlighter-rouge">y_pred</code>라는 단 두가지 인자</strong>만 받을 수 있다는 단점이 존재합니다. 물론 각 방법의 특징에 따라 추가적인 인자들을 활용할 수 있는 방안이 마련되어 있지만, 실제로 loss를 계산하는 메서드는 무조건 위의 두가지 인자만 전달받을 수 있습니다.</p><p>사실 케라스 built-in function인 <code class="language-plaintext highlighter-rouge">model.fit()</code>를 활용하실 계획이 없으시다면, low-level로 loss 함수를 작성하셔도 관계없습니다. 그러나 <code class="language-plaintext highlighter-rouge">model.fit()</code>을 사용하실 계획이시라면 반드시 정해진 형태에 맞춰 코드를 작성해주셔야 합니다. <code class="language-plaintext highlighter-rouge">model.fit()</code> 메서드 내부에 loss와 관련된 부분은 <code class="language-plaintext highlighter-rouge">y_true</code>, <code class="language-plaintext highlighter-rouge">y_pred</code> 두 가지 인자만을 활용하도록 디자인 되어 있기 때문입니다.</p><p><br /></p><h2 id="21-simple-loss-function">2.1   Simple Loss Function</h2><p><a href="https://keras.io/api/losses/#creating-custom-losses">공식 문서</a>에서 제안하는 가장 기본적인 custom keras loss 함수 작성 방법입니다.</p><div class="language-python highlighter-rouge"><div class="code-header" text-data="python"><button data-original-title="Copied!"><i class="far fa-clone"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">my_loss_fn</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>
    <span class="n">squared_difference</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">square</span><span class="p">(</span><span class="n">y_true</span> <span class="o">-</span> <span class="n">y_pred</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tf</span><span class="p">.</span><span class="nf">reduce_mean</span><span class="p">(</span><span class="n">squared_difference</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># Note the `axis=-1`
</span></pre></table></code></div></div><ul><li><code class="language-plaintext highlighter-rouge">y_true</code>: ground_truth 값을 전달합니다.<li><code class="language-plaintext highlighter-rouge">y_pred</code>: 모델의 예측 값을 전달합니다.</ul><p>이 방식의 가장 큰 문제는 <code class="language-plaintext highlighter-rouge">y_true</code>나 <code class="language-plaintext highlighter-rouge">y_pred</code> 이외의 값을 전혀 활용할 수 없다는 것입니다. 만일 loss 함수에 추가적인 파라미터가 요구된다면 이러한 방식의 loss 함수는 사용할 수 없습니다. 다음에 설명드릴 nested loss function이나 loss class 방식을 참고해주세요.</p><p><br /></p><h2 id="22-nested-loss-function">2.2   Nested Loss Function</h2><p>기본적인 loss 함수의 단점을 보완하고 추가적인 파라미터를 전달하기 위해서 다음과 같은 방법으로 custom loss 함수를 작성할 수 있습니다.</p><div class="language-python highlighter-rouge"><div class="code-header" text-data="python"><button data-original-title="Copied!"><i class="far fa-clone"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">my_loss_fn</span><span class="p">(</span><span class="n">threshold</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">inner_fn</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>
        <span class="n">squared_difference</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">square</span><span class="p">(</span><span class="n">y_true</span> <span class="o">-</span> <span class="n">y_pred</span><span class="p">)</span> <span class="o">*</span> <span class="n">threshold</span>
        <span class="k">return</span> <span class="n">tf</span><span class="p">.</span><span class="nf">reduce_mean</span><span class="p">(</span><span class="n">squared_difference</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">inner_fn</span>
</pre></table></code></div></div><p>nested loss function에는 파이썬 클로저(closure)<sup id="fnref:2"><a href="#fn:2" class="footnote" rel="footnote" role="doc-noteref">2</a></sup>의 개념이 활용 되었습니다. 클로저에 대한 자세한 내용은 <a href="https://shoark7.github.io/programming/python/closure-in-python">이 곳</a>을 참고해주세요. 결론적으로 말씀드리면, threshold라는 변수는 inner_fn의 입장에서는 nonlocal 변수인데요. 클로저는 nonlocal 변수인 threshold에 담긴 값을 기억하고 참조할 수 있습니다. 따라서 squared_difference를 계산할 때, inner_fn 함수의 네임 스페이스 바깥 영역에서 선언된 threshold 변수를 참조할 수 있는 것입니다. nested loss 함수는 기본적인 loss 함수에 비해 조금 더 자유롭지만, 저는 개인적으로 클래스를 이용해서 조금 더 깔끔하게 함수를 작성하실 것을 추천드립니다.</p><p><br /></p><h2 id="23-loss-class">2.3   Loss Class</h2><p>개인적으로 custom loss를 작성할 때 가장 선호하는 방식입니다. 가장 가독성이 좋으면서도 외부 파라미터도 활용할 수 있기 때문입니다. 생성자 <code class="language-plaintext highlighter-rouge">__init__()</code>에 필요한 외부 파라미터를 선언해두고 <code class="language-plaintext highlighter-rouge">call()</code> 메서드에 실제 loss를 계산하는 로직을 작성하시면 됩니다.</p><div class="language-python highlighter-rouge"><div class="code-header" text-data="python"><button data-original-title="Copied!"><i class="far fa-clone"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre><td class="rouge-code"><pre><span class="k">class</span> <span class="nc">CustomLoss</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">losses</span><span class="p">.</span><span class="n">Loss</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">param1</span><span class="p">,</span> <span class="n">param2</span><span class="p">,</span> <span class="p">...):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">CustomLoss</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">param1</span> <span class="o">=</span> <span class="n">param1</span>
        <span class="n">self</span><span class="p">.</span><span class="n">param2</span> <span class="o">=</span> <span class="n">param2</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">...</span>
        <span class="k">return</span> <span class="n">loss</span>
</pre></table></code></div></div><p><br /></p><p>loss class 방식은 아래와 같은 keras high-level api와 쉽게 결합 가능합니다.</p><div class="language-python highlighter-rouge"><div class="code-header" text-data="python"><button data-original-title="Copied!"><i class="far fa-clone"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre><td class="rouge-code"><pre><span class="n">loss_fn</span> <span class="o">=</span> <span class="nc">CustomLoss</span><span class="p">(</span><span class="n">param1</span><span class="p">,</span> <span class="n">param2</span><span class="p">)</span>
<span class="bp">...</span>
<span class="n">model</span><span class="p">.</span><span class="nf">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="p">...,</span> <span class="n">loss</span><span class="o">=</span><span class="n">loss_fn</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="nf">fit</span><span class="p">()</span>
</pre></table></code></div></div><p><br /></p><p>그러나 세 가지 방법 모두 여전히 <code class="language-plaintext highlighter-rouge">call()</code> 메서드에 <code class="language-plaintext highlighter-rouge">y_true</code>, <code class="language-plaintext highlighter-rouge">y_pred</code> 두 가지 인자만 전달할 수 있다는 단점은 존재하는데요. <code class="language-plaintext highlighter-rouge">y_true</code> 값이 여러 개일 경우에 문제가 될 수 있습니다. 예를 들어 negative sampling loss를 계산하는 상황을 가정해보겠습니다. 이 경우 <code class="language-plaintext highlighter-rouge">call()</code> 메서드에 <code class="language-plaintext highlighter-rouge">y_true_positive</code>, <code class="language-plaintext highlighter-rouge">y_true_negative</code>, <code class="language-plaintext highlighter-rouge">y_pred</code> 와 같이 인자를 3개를 전달해야 할겁니다.</p><p>인자를 3개를 전달하기 위한 여러 방법을 찾아보았는데, 제가 내린 결론은 <code class="language-plaintext highlighter-rouge">model.fit()</code> 메서드를 사용할 것이라면 인자를 3개 이상 전달할 수 있는 방법은 없다는 것입니다. 그래서 저는 우회적으로 <code class="language-plaintext highlighter-rouge">y_true_positive</code>와 <code class="language-plaintext highlighter-rouge">y_true_negative</code>를 reshape 및 concat 하여 <code class="language-plaintext highlighter-rouge">y_true</code> 인자로 전달한 후 <code class="language-plaintext highlighter-rouge">call()</code> method 내부에서 slice 및 reshape를 통해 negative sampling loss를 계산하는 식으로 로직을 작성했습니다. 혹시 더 좋은 방안이 있다면 제보 부탁드립니다!</p><p><br /></p><h1 id="model-class">Model Class</h1><hr /><p>tf.keras에서 커스텀 모델을 만드는 방법은 크게 3가지입니다.</p><ul><li><a href="https://www.tensorflow.org/guide/keras/sequential_model">Sequential API</a><li><a href="https://www.tensorflow.org/guide/keras/functional">Functional API</a><li><a href="https://www.tensorflow.org/guide/keras/custom_layers_and_models#the_model_class">Model Class (via subclassing)</a></ul><p><br /></p><p>세가지 방법 중 가장 low-level로 custom 할 수 있는 model class를 위주로 설명드리려고 합니다. <a href="https://www.tensorflow.org/guide/keras/custom_layers_and_models#the_model_class">공식 문서</a>에서 제안한 예시를 통해 설명드리겠습니다.</p><p><br /></p><h2 id="31-simple-model-class">3.1   Simple Model Class</h2><div class="language-python highlighter-rouge"><div class="code-header" text-data="python"><button data-original-title="Copied!"><i class="far fa-clone"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
</pre><td class="rouge-code"><pre><span class="k">class</span> <span class="nc">ResNet</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">Model</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">1000</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">ResNet</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">block_1</span> <span class="o">=</span> <span class="nc">ResNetBlock</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">block_2</span> <span class="o">=</span> <span class="nc">ResNetBlock</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">global_pool</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="nc">GlobalAveragePooling2D</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">classifier</span> <span class="o">=</span> <span class="nc">Dense</span><span class="p">(</span><span class="n">num_classes</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">block_1</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">block_2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">global_pool</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">classifier</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></table></code></div></div><ul><li><p><strong>tf.keras.Model 상속</strong></p><p>tf.keras.Model의 상속을 통해서 <code class="language-plaintext highlighter-rouge">model.fit()</code>, <code class="language-plaintext highlighter-rouge">model.compile()</code>, <code class="language-plaintext highlighter-rouge">model.predict()</code> 등의 keras built-in 함수를 자연스럽게 사용할 수 있게 되는데요. <a href="https://github.com/tensorflow/tensorflow/blob/v2.4.1/tensorflow/python/keras/engine/training.py#L138-L2675">여기</a>을 참고 하시면 <code class="language-plaintext highlighter-rouge">tf.keras.Model</code> 클래스가 <code class="language-plaintext highlighter-rouge">fit()</code>, <code class="language-plaintext highlighter-rouge">compile()</code> 등의 메서드를 가지고 있는 것을 볼 수 있습니다.</p><li><p><strong>__init__()</strong></p><p>hyperparameter나 layer class를 정의하는 메서드입니다.</p><li><p><strong>call()</strong></p><p>model의 computation이 일어나는 부분입니다. argument로 inputs를 받아서 custom 원하는 output을 계산하도록 로직을 작성하면 되겠습니다.</p></ul><p><br /></p><h2 id="32-end-to-end-model">3.2   End-to-End Model</h2><p>model class 방식의 가장 큰 장점은 여러 layer class를 조합하여 하나의 큰 모델로 만들 수 있다는 점입니다. 많은 분들께서 결국에는 이와 비슷한 형태로 model class를 만드실 것으로 생각합니다. <a href="">공식 문서</a>의 예제를 조금 간소화하여 설명드릴게요. 아래의 예시는 <em>encode layer class</em>와 <em>decoder layer class</em>를 선언한 뒤에 <em>VariationalAutoEncoder model class</em>로 두 레이어를 조합하는 예시를 보여줍니다. 이와 같이 직접 만드신 layer class나 케라서 built-in layer class를 활용하여 원하시는 모델을 레고 조립하듯이 쌓아 나가시면 되겠습니다.</p><div class="language-python highlighter-rouge"><div class="code-header" text-data="python"><button data-original-title="Copied!"><i class="far fa-clone"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
</pre><td class="rouge-code"><pre><span class="k">class</span> <span class="nc">Encoder</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Layer</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">latent_dim</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="p">...):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">Encoder</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">(...)</span>
        <span class="bp">...</span>
				
    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="n">z_mean</span> <span class="o">=</span> <span class="bp">...</span>
        <span class="n">z_log_var</span> <span class="o">=</span> <span class="bp">...</span>
        <span class="n">z</span> <span class="o">=</span> <span class="bp">...</span>
        <span class="k">return</span> <span class="n">z_mean</span><span class="p">,</span> <span class="n">z_log_var</span><span class="p">,</span> <span class="n">z</span>

<span class="k">class</span> <span class="nc">Decoder</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Layer</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">intermediate_dim</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="p">...):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">Decoder</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">(...)</span>
        <span class="bp">...</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">...</span>
        <span class="n">y</span> <span class="o">=</span> <span class="bp">...</span>
        <span class="k">return</span> <span class="n">y</span>

<span class="k">class</span> <span class="nc">VariationalAutoEncoder</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">Model</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">original_dim</span><span class="p">,</span> <span class="p">...):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">VariationalAutoEncoder</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">(...)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="nc">Encoder</span><span class="p">(...)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="nc">Decoder</span><span class="p">(...)</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">encoder</span><span class="p">(...)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">decoder</span><span class="p">(...)</span>
        <span class="bp">...</span>
        <span class="k">return</span> <span class="n">output</span>
</pre></table></code></div></div><p><br /></p><h1 id="custom-training-loop">Custom Training Loop</h1><hr /><p>training loop를 작성하는 방법은 크게 두가지입니다.</p><ul><li><a href="https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch#using_the_gradienttape_a_first_end-to-end_example">for loop를 활용한 low-level 수준의 코드 작성</a><li><a href="https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit#a_first_simple_example">model class에 train_step() 함수를 오버라이딩</a><sup id="fnref:3"><a href="#fn:3" class="footnote" rel="footnote" role="doc-noteref">3</a></sup>하여 작성</ul><p><br /></p><p>첫번째 방식은 코드를 작성하는 사람의 입맛에 맞게 매우 자유롭게 코드를 작성할 수 있다는 장점이 있지만, 그에 비례해서 코드 작성 과정에서 human error가 발생할 가능성이 높아집니다. 저는 자유롭게 코드를 작성할 수 있으면서도 <code class="language-plaintext highlighter-rouge">model.fit()</code>의 편리한 장점을 취하여 human error를 최소화 할 수 있는 두번째 방식으로 custom training loop를 작성하는 방법을 설명해드리고자 합니다.</p><p><br /></p><p>섹션3에서 언급한 model class에 <code class="language-plaintext highlighter-rouge">train_step()</code> 메서드를 활용하면 됩니다. 코드는 다음과 같은 방식으로 작성하시면 됩니다. 보다 자세한 내용은 <a href="https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit#a_first_simple_example">공식 문서</a>를 참조해주세요.</p><ul><li>tf.keras.Model을 상속 받아 model class를 만듭니다.<li>model class에 <code class="language-plaintext highlighter-rouge">train_step()</code> 메서드를 작성합니다. <code class="language-plaintext highlighter-rouge">model.fit()</code>는 이 메서드를 활용하여 학습이 진행되도록 디자인 되어 있습니다.<li><code class="language-plaintext highlighter-rouge">tf.GradientTape</code> API는 <a href="https://www.tensorflow.org/guide/autodiff#automatic_differentiation_and_gradients">자동 미분(automatic differentiation)</a> 기능을 제공합니다. 따라서 우리는 <code class="language-plaintext highlighter-rouge">train_step()</code> 메서드의 로직 작성 순서만 고려하면 됩니다.</ul><div class="language-python highlighter-rouge"><div class="code-header" text-data="python"><button data-original-title="Copied!"><i class="far fa-clone"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
</pre><td class="rouge-code"><pre><span class="k">class</span> <span class="nc">CustomModel</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">Model</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="bp">...</span>
    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="bp">...</span>
    <span class="k">def</span> <span class="nf">train_step</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
        <span class="c1"># Unpack the data. Its structure depends on your model and on what you pass to `fit()`.
</span>        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">data</span>

        <span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="nc">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
            <span class="c1"># Forward pass
</span>            <span class="n">y_pred</span> <span class="o">=</span> <span class="nf">self</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>  
            <span class="c1"># Compute the loss value (the loss function is configured in `compile()`)
</span>            <span class="n">loss</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">compiled_loss</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">regularization_losses</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">losses</span><span class="p">)</span>

        <span class="c1"># Compute gradients
</span>        <span class="n">trainable_vars</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">trainable_variables</span>
        <span class="n">gradients</span> <span class="o">=</span> <span class="n">tape</span><span class="p">.</span><span class="nf">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">trainable_vars</span><span class="p">)</span>
        <span class="c1"># Update weights
</span>        <span class="n">self</span><span class="p">.</span><span class="n">optimizer</span><span class="p">.</span><span class="nf">apply_gradients</span><span class="p">(</span><span class="nf">zip</span><span class="p">(</span><span class="n">gradients</span><span class="p">,</span> <span class="n">trainable_vars</span><span class="p">))</span>
        <span class="c1"># Update metrics (includes the metric that tracks the loss)
</span>        <span class="n">self</span><span class="p">.</span><span class="n">compiled_metrics</span><span class="p">.</span><span class="nf">update_state</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
        <span class="c1"># Return a dict mapping metric names to current value
</span>        <span class="k">return</span> <span class="p">{</span><span class="n">m</span><span class="p">.</span><span class="n">name</span><span class="p">:</span> <span class="n">m</span><span class="p">.</span><span class="nf">result</span><span class="p">()</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">metrics</span><span class="p">}</span>
</pre></table></code></div></div><p>train_step() 메서드는 다음과 같은 순서로 진행됩니다.</p><ul><li><p><strong>Step1: computation 기록</strong></p><p><code class="language-plaintext highlighter-rouge">tf.GradientTape()</code> 컨텍스트 내부의 연산은 추적되어서 이후 자동 미분이 가능하게 됩니다.</p><li><p><strong>Step2: forward pass</strong></p><p>model class의 <code class="language-plaintext highlighter-rouge">call()</code> 메서드를 활용해서 모델의 예측값을 계산합니다.</p><li><p><strong>Step3: compute loss</strong></p><p><code class="language-plaintext highlighter-rouge">model.compile()</code> 메서드에 전달한 loss 함수로 loss가 계산됩니다.</p><li><p><strong>Step4: compute gradients</strong></p><p>후진 방식 자동 미분(reverse mode differentiation)을 사용해 기록된 연산의 gradient를 계산합니다.</p><li><p><strong>Step5: update weights</strong></p><p>계산된 gradient와 <code class="language-plaintext highlighter-rouge">model.compile()</code> 메서드에 전달한 optimizer를 활용해 weight를 업데이트 합니다.</p><li><p><strong>Step6: update metrics</strong></p><p>현재 step에서의 metrics를 계산합니다.</p><li><p><strong>Step7: return metric dictionary</strong></p><p>training step 마다 계산된 metrics를 progress bar에 출력하기 위해 해당 딕셔너리를 return 합니다.</p></ul><p><br /></p><h1 id="conclusion">Conclusion</h1><p>tf.keras를 커스텀 하는 방법은 매우 다양하고 알아야 되는 개념도 많다는 것을 알게 되었습니다. 특히 저는 keras built-in function들을 최대한 활용할 수 있는 방안을 고민하면서 많은 시행착오를 겪었고 그 결과 제가 체득한 가장 최선의 시나리오(?)를 정리하였습니다. 다만 최대한 쉽게 작성하려고는 했으나, 애초에 알아야할 개념들이 많이 있어서 글 자체의 난이도 조절에는 실패해버린 것 같습니다…</p><p>업무를 어느 정도 일단락 하고 지난 일들을 돌이켜 보면 keras built-in function을 활용한다는 것이 양날이 검이 될 수도 있다는 생각이 들었습니다. 딥러닝 코드에서 어느 정도 정형화된 부분은 keras built-in function을 활용하면 human error를 줄일 수 있는 것은 확실한 장점이라고 생각합니다. 그러나 정말 모든 것을 low-level로 하고 싶은신 분들에게는 어느 정도 규격화된 틀이 있다는 것이 답답하게 느껴질 수도 있을 것 같습니다. 특히 pytorch를 사용하셨던 분들에게는 더욱 크게 다가올 것 같네요.</p><p>아 그리고 이 글에서 담지 못한 이야기들도 많이 있는데요. metric로 커스터마이징이 가능하고, 만들어진 모델을 tf.serving container를 활용해서 API 서버를 구축하시려면 tf.graph에 대한 개념 이해도 필요합니다. 추후에 기회가 되면 이 부분도 다뤄보겠습니다.</p><h1 id="reference">Reference</h1><hr /><ul><li><a href="https://keras.io/examples/keras_recipes/antirectifier/">Simple custom layer example: Antirectifier</a><li><a href="https://keras.io/ko/layers/writing-your-own-keras-layers/">직접 케라스 레이어 만들기</a><li><a href="https://www.tensorflow.org/guide/keras/train_and_evaluate">Training and evaluation with the built-in methods</a><li><a href="https://www.tensorflow.org/guide/keras/custom_layers_and_models">Making new Layers and Models via subclassing</a><li><a href="https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch">Writing a training loop from scratch</a><li><a href="https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit">Customize what happens in Model.fit</a><li><a href="https://keras.io/guides/customizing_what_happens_in_fit/">Customizing what happens in fit()</a><li><a href="https://junstar92.tistory.com/144?category=905975">tensorflow custom loss</a><li><a href="https://keras.io/api/losses/#creating-custom-losses">Losses (keras.io)</a><li><a href="https://www.tensorflow.org/guide/keras/sequential_model">The Sequential model</a><li><a href="https://www.tensorflow.org/guide/autodiff">Introduction to gradients and automatic differentiation</a><li><a href="https://blog.hexabrain.net/347">파이썬 강좌 번외편. 클로저(Closure)</a><li><a href="https://whatisthenext.tistory.com/112?category=761276">클로져(Closure) 이해하기</a><li><a href="https://shoark7.github.io/programming/python/closure-in-python">Python의 Closure에 대해 알아보자</a></ul><p><br /></p><h1 id="footnote">footnote</h1><hr /><div class="footnotes" role="doc-endnotes"><ol><li id="fn:1"><p>서브클래싱(subclassing)이란 객체 지향 프로그래밍(OOP)에 등장하는 개념으로서 <a href="https://epicdevsold.tistory.com/177">구현되어 있는 클래스를 상속하는 것</a>을 말합니다. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p><li id="fn:2"><p><a href="https://shoark7.github.io/programming/python/closure-in-python">클로저(closure)란 자신을 둘러싼 네임 스페이스에 존재하는 변수를 기억할 수 있는 함수</a>를 뜻합니다. 대표적으로 파이썬 데코레이터는 클로저의 개념을 활용한 것입니다. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p><li id="fn:3"><p>오버라이딩(overriding)이란 부모 클래스의 메서드를 자식 클래스에서 재정의하는 것을 말합니다. 자식 클래스의 메서드를 작성할 때 부모 클래스의 메서드와 이름은 같지만 로직을 다르게 하고 싶을 때 사용합니다. <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p></ol></div></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/ml/'>ML</a>, <a href='/categories/tf/'>TF</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/tensorflow/" class="post-tag no-text-decoration" >tensorflow</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=tf.keras 커스텀 하기 - 생각과 고민.&url=https://gguguk.github.io/posts/custom_tensorflow_2/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=tf.keras 커스텀 하기 - 생각과 고민.&u=https://gguguk.github.io/posts/custom_tensorflow_2/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=tf.keras 커스텀 하기 - 생각과 고민.&url=https://gguguk.github.io/posts/custom_tensorflow_2/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" title-succeed="Link copied successfully!"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"> <span>Recent Update</span><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/tsne/">T-SNE 이해하기</a><li><a href="/posts/how_to_work_python/">CS50 - 파이썬이 소스 코드를 실행하는 과정과 원리</a><li><a href="/posts/OIDC/">IRSA의 원리를 파헤쳐보자 4 - OIDC</a><li><a href="/posts/OAuth/">IRSA의 원리를 파헤쳐보자 3 - OAuth2.0</a><li><a href="/posts/admission_webhook/">IRSA의 원리를 파헤쳐보자 1 - K8S Admission Webhook</a></ul></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"> <span class="pl-3 pt-2 mb-2">Contents</span><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/nlp_transfer_learning_history/"><div class="card-body"> <span class="timeago small" >Mar 5, 2020<i class="unloaded">2020-03-05T00:00:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>NLP Transfer Learning History</h3><div class="text-muted small"><p> 김성현 연구원님의 T아카데미 토크ON세미나 ‘딥러닝 기반의 자연어 언어 모델 BERT’ 라는 세미나를 듣고 알게된 정보와 구글링을 통해 알게된 정보를 종합하여 관련 내용을 정리해 보고자 합니다. 1   From word embedding To pretrained language models 1.1   Traditional context-fr...</p></div></div></a></div><div class="card"> <a href="/posts/MAB/"><div class="card-body"> <span class="timeago small" >May 7, 2020<i class="unloaded">2020-05-07T00:00:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>MAB와 Thomson Sampling</h3><div class="text-muted small"><p> MAB를 이해하기 위해 필요한 베타분포(Beta Dist.), 이항분포(Binomial Dist.), 베이지안 추정(Bayesian Estimation), 톰슨 샘플링(Tompson Sampling)과 관련된 내용을 정리하였습니다. 1   A/B 테스트 자동화의 필요성 특정 캠페인을 진행하면서, 어떤 캠페인의 배너가 더 잘 구매 전환을 유도하는...</p></div></div></a></div><div class="card"> <a href="/posts/byte_pair-encoding/"><div class="card-body"> <span class="timeago small" >Jun 5, 2020<i class="unloaded">2020-06-05T00:00:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Byte Pair Encoding</h3><div class="text-muted small"><p> 최근 NLP에서 tokenizer로 많이 사용되고 있는 BPE에 대해서 간단하게 정리해 보겠습니다. 전체코드는 이곳에서 확인해 보실 수 있습니다. 1   Backgroud: Subword Segmentation subword segmentation(단어 분리, 단어 분절)이란, 하나의 단어(혹은 토큰)는 여러 개의 subword의 조합으로 이...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/eigenvalue_decomposition/" class="btn btn-outline-primary" prompt="Older"><p>고유값 분해(Eigenvalue Decompostion)</p></a> <a href="/posts/SVD/" class="btn btn-outline-primary" prompt="Newer"><p>특이값 분해(Singular Value Decompostion)</p></a></div></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2025 <a href="https://github.com/gguguk">Gukwon Koo</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">Trending Tags</h4><a class="post-tag" href="/tags/python/">python</a> <a class="post-tag" href="/tags/retrospective/">retrospective</a> <a class="post-tag" href="/tags/k8s/">k8s</a> <a class="post-tag" href="/tags/kubernetes/">kubernetes</a> <a class="post-tag" href="/tags/nlp/">nlp</a> <a class="post-tag" href="/tags/statistics/">statistics</a> <a class="post-tag" href="/tags/irsa/">irsa</a> <a class="post-tag" href="/tags/ml/">ml</a> <a class="post-tag" href="/tags/mlops/">mlops</a> <a class="post-tag" href="/tags/paper/">paper</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="https://gguguk.github.io{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/lozad/dist/lozad.min.js,npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.16.1,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id=G-8EWVG7CHCY"></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'G-8EWVG7CHCY'); }); </script>
